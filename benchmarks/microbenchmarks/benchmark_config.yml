# Sample configuration for inference kernel benchmarks
quantizations:
  - "baseline"
  - "int8wo"
  - "int4wo-128"
  - "int4wo-128-hqq"
output_dir: "benchmarks/microbenchmarks/results"  # Directory for results and plots
model_params:
  matrix_shapes:
    - name: "custom"
      shapes: [
        [1024, 1024, 1024],  # [m, k, n]
        [2048, 4096, 1024],
        [4096, 4096, 1024]
      ]
  precision: "torch.bfloat16"
  compile: false
  device: "cuda"  # Change this to "cuda", "mps", "xpu", or "cpu" as needed
  model_type: "linear"
