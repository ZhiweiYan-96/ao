README BENCHMARKS
llama 2
20240831225155, tok/s=107.38, mem/s=1418.93 GB/s, peak_mem=13.88 GB, model_size=13.21 GB quant: None, mod: Llama-2-7b-chat-hf, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Llama-2-7b-chat-hf/model.pth --device cuda --precision torch.bfloat16 --compile --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8
20240831225810, tok/s=  9.61, mem/s=  63.67 GB/s, peak_mem= 8.61 GB, model_size= 6.62 GB quant: int8dq, mod: Llama-2-7b-chat-hf, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization int8dq --checkpoint_path ../../../checkpoints/meta-llama/Llama-2-7b-chat-hf/model.pth --device cuda --precision torch.bfloat16 --compile --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8
20240831230013, tok/s=170.83, mem/s=1131.18 GB/s, peak_mem= 8.95 GB, model_size= 6.62 GB quant: int8wo, mod: Llama-2-7b-chat-hf, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization int8wo --checkpoint_path ../../../checkpoints/meta-llama/Llama-2-7b-chat-hf/model.pth --device cuda --precision torch.bfloat16 --compile --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8
20240910152454, tok/s=117.89, mem/s= 584.57 GB/s, peak_mem= 6.52 GB, model_size= 4.96 GB quant: fp6, mod: Llama-2-7b-chat-hf, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.float16, device: cuda repro: python generate.py --quantization fp6 --checkpoint_path ../../../checkpoints/meta-llama/Llama-2-7b-chat-hf/model.pth --device cuda --precision torch.float16 --compile --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8
20240831230205, tok/s=201.14, mem/s= 751.42 GB/s, peak_mem= 4.87 GB, model_size= 3.74 GB quant: int4wo-64, mod: Llama-2-7b-chat-hf, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization int4wo-64 --checkpoint_path ../../../checkpoints/meta-llama/Llama-2-7b-chat-hf/model.pth --device cuda --precision torch.bfloat16 --compile --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8
20240831230736, tok/s=177.45, mem/s=1194.35 GB/s, peak_mem= 8.64 GB, model_size= 6.73 GB quant: autoquant, mod: Llama-2-7b-chat-hf, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization autoquant --checkpoint_path ../../../checkpoints/meta-llama/Llama-2-7b-chat-hf/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8
20240902100527, tok/s=209.19, mem/s= 804.32 GB/s, peak_mem= 4.89 GB, model_size= 3.84 GB quant: autoquant-int4, mod: Llama-2-7b-chat-hf, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization autoquant-int4 --checkpoint_path ../../../checkpoints/meta-llama/Llama-2-7b-chat-hf/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8

llama 3
20240831232535, tok/s= 95.64, mem/s=1435.54 GB/s, peak_mem=16.43 GB, model_size=15.01 GB quant: None, mod: Meta-Llama-3-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3-8B/model.pth --device cuda --precision torch.bfloat16 --compile --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8
20240831233224, tok/s=  8.61, mem/s=  64.75 GB/s, peak_mem= 9.24 GB, model_size= 7.52 GB quant: int8dq, mod: Meta-Llama-3-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization int8dq --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3-8B/model.pth --device cuda --precision torch.bfloat16 --compile --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8
20240831233853, tok/s=153.03, mem/s=1150.80 GB/s, peak_mem=10.42 GB, model_size= 7.52 GB quant: int8wo, mod: Meta-Llama-3-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization int8wo --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3-8B/model.pth --device cuda --precision torch.bfloat16 --compile --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8
20240910153353, tok/s=161.58, mem/s= 910.02 GB/s, peak_mem= 7.72 GB, model_size= 5.63 GB quant: fp6, mod: Meta-Llama-3-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.float16, device: cuda repro: python generate.py --quantization fp6 --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3-8B/model.pth --device cuda --precision torch.float16 --compile --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8
20240831234218, tok/s=180.80, mem/s= 763.33 GB/s, peak_mem= 6.88 GB, model_size= 4.22 GB quant: int4wo-64, mod: Meta-Llama-3-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization int4wo-64 --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3-8B/model.pth --device cuda --precision torch.bfloat16 --compile --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8
20240831235355, tok/s=158.10, mem/s=1193.24 GB/s, peak_mem=10.04 GB, model_size= 7.55 GB quant: autoquant, mod: Meta-Llama-3-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization autoquant --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8
20240902101015, tok/s=188.41, mem/s= 800.58 GB/s, peak_mem= 7.14 GB, model_size= 4.25 GB quant: autoquant-int4, mod: Meta-Llama-3-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization autoquant-int4 --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8

KV CACHE QUANTIZATION:
20240826161508, tok/s= 19.71, mem/s= 295.80 GB/s, peak_mem=17.86 GB, model_size=15.01 GB quant: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: False, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8 --cache_size 8192
20240826161747, tok/s= 13.52, mem/s= 202.96 GB/s, peak_mem=17.52 GB, model_size=15.01 GB quant: None, mod: Meta-Llama-3.1-8B, kv_quant: True, compile: False, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8 --cache_size 8192--kv_cache_quantization
20240826162028, tok/s= 13.30, mem/s= 199.66 GB/s, peak_mem=17.47 GB, model_size=15.01 GB quant: None, mod: Meta-Llama-3.1-8B, kv_quant: True, compile: False, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8 --cache_size 8192--kv_cache_quantization --linear_causal_mask
20240826162318, tok/s= 12.54, mem/s= 188.22 GB/s, peak_mem=19.81 GB, model_size=15.01 GB quant: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: False, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8 --cache_size 16384
20240826162620, tok/s= 10.67, mem/s= 160.12 GB/s, peak_mem=18.75 GB, model_size=15.01 GB quant: None, mod: Meta-Llama-3.1-8B, kv_quant: True, compile: False, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8 --cache_size 16384--kv_cache_quantization
20240826162920, tok/s= 10.57, mem/s= 158.67 GB/s, peak_mem=18.48 GB, model_size=15.01 GB quant: None, mod: Meta-Llama-3.1-8B, kv_quant: True, compile: False, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8 --cache_size 16384--kv_cache_quantization --linear_causal_mask
20240826163307, tok/s=  7.11, mem/s= 106.75 GB/s, peak_mem=23.83 GB, model_size=15.01 GB quant: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: False, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8 --cache_size 32768
20240826163710, tok/s=  6.33, mem/s=  94.98 GB/s, peak_mem=21.72 GB, model_size=15.01 GB quant: None, mod: Meta-Llama-3.1-8B, kv_quant: True, compile: False, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8 --cache_size 32768--kv_cache_quantization
20240826164117, tok/s=  6.20, mem/s=  93.02 GB/s, peak_mem=20.64 GB, model_size=15.01 GB quant: None, mod: Meta-Llama-3.1-8B, kv_quant: True, compile: False, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8 --cache_size 32768--kv_cache_quantization --linear_causal_mask
20240826164715, tok/s=  3.72, mem/s=  55.84 GB/s, peak_mem=33.50 GB, model_size=15.01 GB quant: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: False, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8 --cache_size 65536
20240826165343, tok/s=  3.32, mem/s=  49.90 GB/s, peak_mem=29.54 GB, model_size=15.01 GB quant: None, mod: Meta-Llama-3.1-8B, kv_quant: True, compile: False, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8 --cache_size 65536--kv_cache_quantization
20240826170011, tok/s=  3.31, mem/s=  49.71 GB/s, peak_mem=25.24 GB, model_size=15.01 GB quant: None, mod: Meta-Llama-3.1-8B, kv_quant: True, compile: False, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8 --cache_size 65536--kv_cache_quantization --linear_causal_mask
20240826171015, tok/s=  1.95, mem/s=  29.21 GB/s, peak_mem=59.27 GB, model_size=15.01 GB quant: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: False, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8 --cache_size 131072
20240826172121, tok/s=  1.73, mem/s=  26.02 GB/s, peak_mem=52.62 GB, model_size=15.01 GB quant: None, mod: Meta-Llama-3.1-8B, kv_quant: True, compile: False, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8 --cache_size 131072--kv_cache_quantization
20240826173230, tok/s=  1.73, mem/s=  25.95 GB/s, peak_mem=34.18 GB, model_size=15.01 GB quant: None, mod: Meta-Llama-3.1-8B, kv_quant: True, compile: False, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8 --cache_size 131072--kv_cache_quantization --linear_causal_mask

OTHER BENCHMARKS
20240831224311, tok/s= 26.75, mem/s= 707.01 GB/s, peak_mem=27.23 GB, model_size=26.43 GB quant: None, mod: Llama-2-7b-chat-hf, kv_quant: False, compile: False, compile_prefill: False, dtype: torch.float32, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Llama-2-7b-chat-hf/model.pth --device cuda --precision torch.float32 --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8
20240831224512, tok/s= 22.97, mem/s= 303.53 GB/s, peak_mem=13.64 GB, model_size=13.21 GB quant: None, mod: Llama-2-7b-chat-hf, kv_quant: False, compile: False, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Llama-2-7b-chat-hf/model.pth --device cuda --precision torch.bfloat16 --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8
20240831224958, tok/s=108.48, mem/s=1433.57 GB/s, peak_mem=13.90 GB, model_size=13.21 GB quant: None, mod: Llama-2-7b-chat-hf, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Llama-2-7b-chat-hf/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8
20240910004030, tok/s= 22.72, mem/s= 112.66 GB/s, peak_mem=10.41 GB, model_size= 4.96 GB quant: fp6, mod: Llama-2-7b-chat-hf, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization fp6 --checkpoint_path ../../../checkpoints/meta-llama/Llama-2-7b-chat-hf/model.pth --device cuda --precision torch.bfloat16 --compile --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8
20240910004539, tok/s= 50.99, mem/s= 200.08 GB/s, peak_mem= 6.29 GB, model_size= 3.92 GB quant: uintx-4-64, mod: Llama-2-7b-chat-hf, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization uintx-4-64 --checkpoint_path ../../../checkpoints/meta-llama/Llama-2-7b-chat-hf/model.pth --device cuda --precision torch.bfloat16 --compile --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8
20240910005147, tok/s= 40.25, mem/s= 265.95 GB/s, peak_mem= 9.24 GB, model_size= 6.61 GB quant: uintx-2-8, mod: Llama-2-7b-chat-hf, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization uintx-2-8 --checkpoint_path ../../../checkpoints/meta-llama/Llama-2-7b-chat-hf/model.pth --device cuda --precision torch.bfloat16 --compile --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8
20240910110554, tok/s=245.07, mem/s= 657.93 GB/s, peak_mem= 4.05 GB, model_size= 2.68 GB quant: sparse-marlin, mod: Llama-2-7b-chat-hf, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.float16, device: cuda repro: python generate.py --quantization sparse-marlin --checkpoint_path ../../../checkpoints/meta-llama/Llama-2-7b-chat-hf/model.pth --device cuda --precision torch.float16 --compile --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8

20240831231514, tok/s= 26.54, mem/s= 796.59 GB/s, peak_mem=32.34 GB, model_size=30.02 GB quant: None, mod: Meta-Llama-3-8B, kv_quant: False, compile: False, compile_prefill: False, dtype: torch.float32, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3-8B/model.pth --device cuda --precision torch.float32 --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8
20240831231725, tok/s= 23.67, mem/s= 355.33 GB/s, peak_mem=16.19 GB, model_size=15.01 GB quant: None, mod: Meta-Llama-3-8B, kv_quant: False, compile: False, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3-8B/model.pth --device cuda --precision torch.bfloat16 --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8
20240831232327, tok/s= 96.59, mem/s=1449.85 GB/s, peak_mem=16.43 GB, model_size=15.01 GB quant: None, mod: Meta-Llama-3-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8
20240910005537, tok/s= 20.22, mem/s= 113.89 GB/s, peak_mem=23.17 GB, model_size= 5.63 GB quant: fp6, mod: Meta-Llama-3-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization fp6 --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3-8B/model.pth --device cuda --precision torch.bfloat16 --compile --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8
20240910010056, tok/s= 47.85, mem/s= 213.24 GB/s, peak_mem=11.85 GB, model_size= 4.46 GB quant: uintx-4-64, mod: Meta-Llama-3-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization uintx-4-64 --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3-8B/model.pth --device cuda --precision torch.bfloat16 --compile --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8
20240910010647, tok/s= 34.83, mem/s= 261.42 GB/s, peak_mem=14.99 GB, model_size= 7.51 GB quant: uintx-2-8, mod: Meta-Llama-3-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization uintx-2-8 --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3-8B/model.pth --device cuda --precision torch.bfloat16 --compile --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8
20240910110958, tok/s=223.95, mem/s= 682.88 GB/s, peak_mem= 5.59 GB, model_size= 3.05 GB quant: sparse-marlin, mod: Meta-Llama-3-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.float16, device: cuda repro: python generate.py --quantization sparse-marlin --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3-8B/model.pth --device cuda --precision torch.float16 --compile --num_samples 5 --max_new_tokens 200 --top_k 200 --temperature 0.8

20241202193644, tok/s=146.02, tok/s_decode=149.43, ttft=0.0304, mem/s=1929.58 GB/s, peak_mem=13.91 GB, model_size=13.21 GB quant: None, sparse: None, mod: Llama-2-7b-chat-hf, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Llama-2-7b-chat-hf/model.pth --device cuda --precision torch.bfloat16 --compile --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241202194134, tok/s= 86.11, tok/s_decode=113.42, ttft=0.5584, mem/s=1292.50 GB/s, peak_mem=35.37 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8000--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241202194419, tok/s= 43.40, tok/s_decode= 51.81, ttft=0.7466, mem/s= 326.39 GB/s, peak_mem=20.18 GB, model_size= 7.52 GB quant: int8dq, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization int8dq --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8000--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241202194617, tok/s= 80.43, tok/s_decode=113.02, ttft=0.5597, mem/s=1207.28 GB/s, peak_mem=35.37 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8000--profile baseline --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241202194851, tok/s= 41.28, tok/s_decode= 51.75, ttft=0.7454, mem/s= 310.46 GB/s, peak_mem=20.18 GB, model_size= 7.52 GB quant: int8dq, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization int8dq --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8000--profile int8_dynamic --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241202195622, tok/s= 80.70, tok/s_decode=113.08, ttft=0.5573, mem/s=1211.31 GB/s, peak_mem=35.37 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8000--profile baseline --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241202195903, tok/s= 41.06, tok/s_decode= 51.73, ttft=0.7466, mem/s= 308.78 GB/s, peak_mem=20.18 GB, model_size= 7.52 GB quant: int8dq, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization int8dq --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8000--profile int8_dynamic --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241202200007, tok/s= 79.79, tok/s_decode=113.07, ttft=0.5755, mem/s=1197.63 GB/s, peak_mem=35.37 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8000--profile baseline --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241202200245, tok/s= 41.25, tok/s_decode= 51.74, ttft=0.7457, mem/s= 310.18 GB/s, peak_mem=20.18 GB, model_size= 7.52 GB quant: int8dq, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization int8dq --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8000--profile int8_dynamic --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241202201125, tok/s= 80.44, tok/s_decode=113.02, ttft=0.5584, mem/s=1207.42 GB/s, peak_mem=35.37 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8000--profile baseline --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241202201500, tok/s= 80.36, tok/s_decode=113.04, ttft=0.5592, mem/s=1206.19 GB/s, peak_mem=35.37 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8000--profile baseline --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241202202522, tok/s= 80.61, tok/s_decode=113.05, ttft=0.5575, mem/s=1209.87 GB/s, peak_mem=35.37 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8000--profile baseline --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241202202934, tok/s= 83.51, tok/s_decode=119.05, ttft=0.5601, mem/s=1253.49 GB/s, peak_mem=35.63 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8000--profile baseline --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241202204824, tok/s= 41.04, tok/s_decode= 51.62, ttft=0.7461, mem/s= 308.63 GB/s, peak_mem=20.43 GB, model_size= 7.52 GB quant: int8dq, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization int8dq --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8000--profile int8_dynamic --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241202210208, tok/s= 79.85, tok/s_decode=112.50, ttft=0.5602, mem/s=1198.49 GB/s, peak_mem=35.64 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8000--profile baseline --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241202213408, tok/s= 39.04, tok/s_decode= 51.73, ttft=1.2340, mem/s= 293.57 GB/s, peak_mem=21.06 GB, model_size= 7.52 GB quant: int8dq, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization int8dq --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8000--profile int8_dynamic --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241202213900, tok/s= 73.07, tok/s_decode=113.58, ttft=0.9211, mem/s=1096.82 GB/s, peak_mem=35.84 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8000--profile baseline --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241202214136, tok/s= 42.48, tok/s_decode= 51.73, ttft=0.6085, mem/s= 319.42 GB/s, peak_mem=21.02 GB, model_size= 7.52 GB quant: int8dq, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization int8dq --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8000--profile int8_dynamic --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241202214854, tok/s=104.37, tok/s_decode=170.93, ttft=0.8567, mem/s= 784.87 GB/s, peak_mem=29.58 GB, model_size= 7.52 GB quant: int8wo, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization int8wo --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8000--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241202215512, tok/s= 67.92, tok/s_decode= 96.21, ttft=1.2889, mem/s= 367.20 GB/s, peak_mem=19.36 GB, model_size= 5.41 GB quant: int8dq, sparse: semi-structured, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization int8dq --sparsity semi-structured --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8000--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241202220507, tok/s= 94.09, tok/s_decode=143.12, ttft=0.9541, mem/s= 706.20 GB/s, peak_mem=22.45 GB, model_size= 7.51 GB quant: float8dq, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8dq --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8000--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241202221235, tok/s=102.33, tok/s_decode=143.27, ttft=0.5682, mem/s= 768.02 GB/s, peak_mem=22.11 GB, model_size= 7.51 GB quant: float8dq, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8dq --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8000--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241202223133, tok/s= 83.46, tok/s_decode=128.63, ttft=1.4323, mem/s= 626.37 GB/s, peak_mem=22.45 GB, model_size= 7.51 GB quant: float8dq, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8dq --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8000--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241202225401, tok/s= 83.45, tok/s_decode=129.94, ttft=1.3899, mem/s= 626.29 GB/s, peak_mem=21.97 GB, model_size= 7.51 GB quant: float8dq, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8dq --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8000--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241202235122, tok/s= 77.53, tok/s_decode=121.18, ttft=1.4155, mem/s= 417.98 GB/s, peak_mem=20.87 GB, model_size= 5.39 GB quant: float8dq, sparse: semi-structured, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8dq --sparsity semi-structured --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8000--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203000121, tok/s= 81.90, tok/s_decode=126.06, ttft=1.3884, mem/s= 614.64 GB/s, peak_mem=22.44 GB, model_size= 7.51 GB quant: float8dq, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8dq --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8000--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203000721, tok/s= 77.88, tok/s_decode=112.40, ttft=0.8888, mem/s=1168.96 GB/s, peak_mem=35.82 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8000--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203002414, tok/s= 79.29, tok/s_decode=112.40, ttft=0.7958, mem/s=1190.08 GB/s, peak_mem=35.82 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8000--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203003437, tok/s= 74.83, tok/s_decode=107.60, ttft=0.9161, mem/s=1123.19 GB/s, peak_mem=36.70 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203004730, tok/s= 81.74, tok/s_decode=124.87, ttft=1.2614, mem/s= 613.47 GB/s, peak_mem=22.92 GB, model_size= 7.51 GB quant: float8dq, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8dq --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203010003, tok/s= 83.06, tok/s_decode=126.45, ttft=1.0241, mem/s= 447.77 GB/s, peak_mem=20.82 GB, model_size= 5.39 GB quant: float8dq, sparse: semi-structured, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8dq --sparsity semi-structured --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203010542, tok/s= 76.34, tok/s_decode=107.61, ttft=0.8106, mem/s=1145.82 GB/s, peak_mem=36.70 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203011245, tok/s= 89.32, tok/s_decode=134.10, ttft=0.9042, mem/s= 670.40 GB/s, peak_mem=22.85 GB, model_size= 7.51 GB quant: float8dq, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8dq --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203011526, tok/s= 91.94, tok/s_decode=132.66, ttft=0.6832, mem/s= 495.66 GB/s, peak_mem=20.72 GB, model_size= 5.39 GB quant: float8dq, sparse: semi-structured, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8dq --sparsity semi-structured --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203011721, tok/s= 79.54, tok/s_decode=107.76, ttft=0.6622, mem/s=1193.88 GB/s, peak_mem=36.31 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203011939, tok/s= 94.32, tok/s_decode=133.71, ttft=0.6459, mem/s= 707.93 GB/s, peak_mem=22.82 GB, model_size= 7.51 GB quant: float8dq, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8dq --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203012322, tok/s= 91.09, tok/s_decode=131.42, ttft=0.6925, mem/s= 491.10 GB/s, peak_mem=20.72 GB, model_size= 5.39 GB quant: float8dq, sparse: semi-structured, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8dq --sparsity semi-structured --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203013213, tok/s=122.02, tok/s_decode=125.66, ttft=0.2617, mem/s=1831.55 GB/s, peak_mem=17.92 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 1024--num_samples 5 --max_new_tokens 1024 --batch_size 1 --top_k 200 --temperature 0.8
20241203013648, tok/s=123.78, tok/s_decode=125.68, ttft=0.1264, mem/s=1857.93 GB/s, peak_mem=17.65 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 1024--num_samples 5 --max_new_tokens 1024 --batch_size 1 --top_k 200 --temperature 0.8
20241203014219, tok/s=124.15, tok/s_decode=125.68, ttft=0.1004, mem/s=1863.50 GB/s, peak_mem=17.65 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 1024--num_samples 5 --max_new_tokens 1024 --batch_size 1 --top_k 200 --temperature 0.8
20241203014628, tok/s=109.11, tok/s_decode=119.30, ttft=0.4335, mem/s=1637.76 GB/s, peak_mem=22.72 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 4096--num_samples 5 --max_new_tokens 512 --batch_size 1 --top_k 200 --temperature 0.8
20241203021750, tok/s= 49.67, tok/s_decode=114.00, ttft=2.7212, mem/s= 372.76 GB/s, peak_mem=52.53 GB, model_size= 7.51 GB quant: float8dq, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8dq --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 16384--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203022937, tok/s= 50.58, tok/s_decode=116.29, ttft=2.6217, mem/s= 272.67 GB/s, peak_mem=49.77 GB, model_size= 5.39 GB quant: float8dq, sparse: semi-structured, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8dq --sparsity semi-structured --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 16384--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203024606, tok/s= 86.77, tok/s_decode=131.50, ttft=1.0151, mem/s= 651.25 GB/s, peak_mem=22.79 GB, model_size= 7.51 GB quant: float8dq, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8dq --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203025801, tok/s= 80.87, tok/s_decode=124.10, ttft=1.3160, mem/s= 436.00 GB/s, peak_mem=20.79 GB, model_size= 5.39 GB quant: float8dq, sparse: semi-structured, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8dq --sparsity semi-structured --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203030335, tok/s= 97.04, tok/s_decode=137.90, ttft=0.6223, mem/s= 728.31 GB/s, peak_mem=22.55 GB, model_size= 7.51 GB quant: float8dq, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8dq --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203030726, tok/s= 94.41, tok/s_decode=134.74, ttft=0.6539, mem/s= 708.59 GB/s, peak_mem=22.55 GB, model_size= 7.51 GB quant: float8dq, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8dq --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203031541, tok/s= 83.52, tok/s_decode=124.76, ttft=1.0326, mem/s= 450.28 GB/s, peak_mem=20.77 GB, model_size= 5.39 GB quant: float8dq, sparse: semi-structured, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8dq --sparsity semi-structured --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203032101, tok/s= 96.18, tok/s_decode=136.09, ttft=0.6205, mem/s= 721.86 GB/s, peak_mem=22.55 GB, model_size= 7.51 GB quant: float8dq, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8dq --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203032430, tok/s= 95.43, tok/s_decode=135.77, ttft=0.6393, mem/s= 514.47 GB/s, peak_mem=20.63 GB, model_size= 5.39 GB quant: float8dq, sparse: semi-structured, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8dq --sparsity semi-structured --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203033413, tok/s= 96.00, tok/s_decode=135.53, ttft=0.6191, mem/s= 720.47 GB/s, peak_mem=22.55 GB, model_size= 7.51 GB quant: float8dq, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8dq --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203034438, tok/s= 80.67, tok/s_decode=123.41, ttft=1.3384, mem/s= 434.93 GB/s, peak_mem=20.54 GB, model_size= 5.39 GB quant: float8dq, sparse: semi-structured, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8dq --sparsity semi-structured --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203042913, tok/s= 97.51, tok/s_decode=138.65, ttft=0.6192, mem/s= 731.85 GB/s, peak_mem=22.35 GB, model_size= 7.51 GB quant: float8dq, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8dq --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203043240, tok/s= 93.85, tok/s_decode=134.61, ttft=0.6758, mem/s= 505.96 GB/s, peak_mem=20.51 GB, model_size= 5.39 GB quant: float8dq, sparse: semi-structured, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8dq --sparsity semi-structured --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203062351, tok/s= 83.39, tok/s_decode=136.27, ttft=0.6134, mem/s= 625.82 GB/s, peak_mem=22.35 GB, model_size= 7.51 GB quant: float8dq, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8dq --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--profile dense_fp8 --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203062722, tok/s= 80.46, tok/s_decode=131.96, ttft=0.6392, mem/s= 433.76 GB/s, peak_mem=20.51 GB, model_size= 5.39 GB quant: float8dq, sparse: semi-structured, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8dq --sparsity semi-structured --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--profile sparse_fp8 --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203063904, tok/s= 95.46, tok/s_decode=135.93, ttft=0.6229, mem/s= 716.46 GB/s, peak_mem=14.36 GB, model_size= 7.51 GB quant: float8dq, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8dq --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203064036, tok/s= 90.40, tok/s_decode=133.68, ttft=0.7155, mem/s= 487.38 GB/s, peak_mem=13.10 GB, model_size= 5.39 GB quant: float8dq, sparse: semi-structured, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8dq --sparsity semi-structured --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203064208, tok/s= 81.12, tok/s_decode=107.84, ttft=0.6093, mem/s=1217.52 GB/s, peak_mem=21.27 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203064456, tok/s= 74.87, tok/s_decode=107.70, ttft=0.9155, mem/s=1123.82 GB/s, peak_mem=36.31 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203064628, tok/s= 70.76, tok/s_decode=107.49, ttft=0.6105, mem/s=1062.11 GB/s, peak_mem=21.27 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --prefill_size 8192--profile eager --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203064819, tok/s= 68.57, tok/s_decode=107.31, ttft=0.6970, mem/s=1029.17 GB/s, peak_mem=36.21 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--profile baseline --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203070643, tok/s= 71.06, tok/s_decode=107.58, ttft=0.6085, mem/s=1066.61 GB/s, peak_mem=21.27 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --prefill_size 8192--profile eager --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203070823, tok/s= 69.62, tok/s_decode=107.50, ttft=0.6657, mem/s=1044.98 GB/s, peak_mem=36.17 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--profile baseline --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203071318, tok/s= 70.98, tok/s_decode=107.54, ttft=0.6091, mem/s=1065.47 GB/s, peak_mem=21.27 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --prefill_size 8192--profile eager --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203071458, tok/s= 69.59, tok/s_decode=107.46, ttft=0.6660, mem/s=1044.58 GB/s, peak_mem=36.17 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--profile baseline --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203071939, tok/s= 70.31, tok/s_decode=106.60, ttft=0.6110, mem/s=1055.28 GB/s, peak_mem=21.53 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --prefill_size 8192--profile eager --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203072502, tok/s= 63.93, tok/s_decode=107.26, ttft=0.9996, mem/s= 959.63 GB/s, peak_mem=36.70 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--profile baseline --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203073952, tok/s= 70.76, tok/s_decode=107.50, ttft=0.6100, mem/s=1062.06 GB/s, peak_mem=21.27 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --prefill_size 8192--profile eager --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203074137, tok/s= 68.64, tok/s_decode=107.39, ttft=0.6978, mem/s=1030.28 GB/s, peak_mem=36.17 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--profile baseline --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203074559, tok/s= 70.43, tok/s_decode=107.28, ttft=0.6109, mem/s=1057.21 GB/s, peak_mem=21.53 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --prefill_size 8192--profile eager --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203075005, tok/s= 66.13, tok/s_decode=107.21, ttft=0.8219, mem/s= 992.61 GB/s, peak_mem=36.70 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--profile baseline --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203075243, tok/s= 36.53, tok/s_decode= 44.92, ttft=0.6107, mem/s= 548.34 GB/s, peak_mem=21.80 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --prefill_size 8192--profile eager --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203075441, tok/s= 36.09, tok/s_decode= 44.92, ttft=0.6971, mem/s= 541.77 GB/s, peak_mem=36.38 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--profile baseline --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203075928, tok/s= 36.48, tok/s_decode= 44.91, ttft=0.6098, mem/s= 547.49 GB/s, peak_mem=21.39 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --prefill_size 8192--profile eager --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203080128, tok/s= 35.99, tok/s_decode= 44.89, ttft=0.6971, mem/s= 540.22 GB/s, peak_mem=36.38 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--profile baseline --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203080312, tok/s= 36.52, tok/s_decode= 44.92, ttft=0.6104, mem/s= 548.20 GB/s, peak_mem=21.39 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --prefill_size 8192--profile eager --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203080504, tok/s= 36.23, tok/s_decode= 44.90, ttft=0.6717, mem/s= 543.84 GB/s, peak_mem=36.17 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--profile baseline --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203084232, tok/s=304.00, tok/s_decode=160256416.00, ttft=0.6102, mem/s=4562.94 GB/s, peak_mem=21.22 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --prefill_size 8192--profile eager --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203084335, tok/s=298.39, tok/s_decode=130560408.00, ttft=0.6867, mem/s=4478.75 GB/s, peak_mem=36.12 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--profile baseline --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203084438, tok/s=304.84, tok/s_decode=157051280.00, ttft=0.6095, mem/s=4575.66 GB/s, peak_mem=21.22 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --prefill_size 8192--profile eager --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203084817, tok/s=276.18, tok/s_decode=128172584.00, ttft=0.9259, mem/s=4145.41 GB/s, peak_mem=36.69 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--profile baseline --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203084851, tok/s=303.63, tok/s_decode=154725600.00, ttft=0.6124, mem/s=4557.50 GB/s, peak_mem=21.22 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --prefill_size 8192--profile eager --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203084952, tok/s=298.76, tok/s_decode=130954984.00, ttft=0.6855, mem/s=4484.29 GB/s, peak_mem=36.15 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--profile baseline --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203085540, tok/s=304.50, tok/s_decode=160256416.00, ttft=0.6106, mem/s=4570.57 GB/s, peak_mem=21.22 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --prefill_size 8192--profile eager --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203085646, tok/s=299.53, tok/s_decode=130837912.00, ttft=0.6826, mem/s=4495.83 GB/s, peak_mem=36.15 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--profile baseline --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203091728, tok/s=304.11, tok/s_decode=154725600.00, ttft=0.6114, mem/s=4564.70 GB/s, peak_mem=21.22 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --prefill_size 8192--profile eager --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203091829, tok/s=300.12, tok/s_decode=131547376.00, ttft=0.6836, mem/s=4504.82 GB/s, peak_mem=36.15 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--profile baseline --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203092036, tok/s=303.66, tok/s_decode=160256416.00, ttft=0.6116, mem/s=4557.94 GB/s, peak_mem=21.22 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --prefill_size 8192--profile eager --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203092139, tok/s=306.13, tok/s_decode=126722896.00, ttft=0.6583, mem/s=4594.91 GB/s, peak_mem=36.15 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--profile baseline --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203092911, tok/s=303.97, tok/s_decode=160256416.00, ttft=0.6113, mem/s=4562.53 GB/s, peak_mem=21.22 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --prefill_size 8192--profile eager --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203093013, tok/s=306.16, tok/s_decode=131860096.00, ttft=0.6579, mem/s=4595.41 GB/s, peak_mem=36.15 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--profile baseline --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203093337, tok/s=301.96, tok/s_decode=124827632.00, ttft=0.6616, mem/s=4532.36 GB/s, peak_mem=36.15 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --prefill_size 8192--profile eager --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203093438, tok/s=306.13, tok/s_decode=128742512.00, ttft=0.6572, mem/s=4594.97 GB/s, peak_mem=36.15 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--profile baseline --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203094135, tok/s=208.30, tok/s_decode=123622352.00, ttft=1.1727, mem/s=3126.48 GB/s, peak_mem=28.12 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --prefill_size 8192--profile eager --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203094304, tok/s=230.76, tok/s_decode=128776432.00, ttft=0.8683, mem/s=3463.67 GB/s, peak_mem=27.66 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--profile baseline --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203094638, tok/s=209.25, tok/s_decode=130194512.00, ttft=1.1504, mem/s=3140.83 GB/s, peak_mem=27.66 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --prefill_size 8192--profile eager --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203094808, tok/s=225.99, tok/s_decode=127778880.00, ttft=0.8982, mem/s=3392.06 GB/s, peak_mem=27.66 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--profile baseline --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203095405, tok/s=419.13, tok/s_decode=127399232.00, ttft=0.5882, mem/s=6291.02 GB/s, peak_mem=21.70 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --prefill_size 8192--profile eager --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203095508, tok/s=452.44, tok/s_decode=128048784.00, ttft=0.4514, mem/s=6790.99 GB/s, peak_mem=21.17 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--profile baseline --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203100153, tok/s= 74.64, tok/s_decode=107.67, ttft=0.4817, mem/s=1120.27 GB/s, peak_mem=21.53 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --prefill_size 8192--profile eager --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203100428, tok/s= 71.87, tok/s_decode=107.53, ttft=0.6142, mem/s=1078.83 GB/s, peak_mem=21.50 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--profile baseline --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203100607, tok/s= 74.38, tok/s_decode=107.37, ttft=0.4834, mem/s=1116.40 GB/s, peak_mem=21.54 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --prefill_size 8192--profile eager --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203100747, tok/s= 75.24, tok/s_decode=107.33, ttft=0.4621, mem/s=1129.39 GB/s, peak_mem=21.23 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--profile baseline --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203101021, tok/s= 85.52, tok/s_decode=107.80, ttft=0.4816, mem/s=1283.70 GB/s, peak_mem=21.27 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203101157, tok/s= 86.46, tok/s_decode=107.69, ttft=0.4609, mem/s=1297.72 GB/s, peak_mem=21.23 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203102333, tok/s=104.68, tok/s_decode=157.59, ttft=0.8978, mem/s= 787.22 GB/s, peak_mem=16.53 GB, model_size= 7.52 GB quant: int8wo, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization int8wo --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203103025, tok/s=100.52, tok/s_decode=134.07, ttft=0.4971, mem/s= 754.41 GB/s, peak_mem=14.84 GB, model_size= 7.51 GB quant: float8dq, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8dq --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203103648, tok/s=101.27, tok/s_decode=150.37, ttft=0.8948, mem/s= 760.33 GB/s, peak_mem=16.46 GB, model_size= 7.51 GB quant: float8wo, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8wo --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203104136, tok/s= 34.85, tok/s_decode=169.09, ttft=4.6430, mem/s= 147.14 GB/s, peak_mem=11.42 GB, model_size= 4.22 GB quant: int4wo-64, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization int4wo-64 --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203104812, tok/s= 96.43, tok/s_decode=138.61, ttft=0.6866, mem/s= 374.79 GB/s, peak_mem=10.68 GB, model_size= 3.89 GB quant: sparse-marlin, sparse: semi-structured, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.float16, device: cuda repro: python generate.py --quantization sparse-marlin --sparsity semi-structured --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.float16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203154759, tok/s= 85.50, tok/s_decode=107.71, ttft=0.4808, mem/s=1283.28 GB/s, peak_mem=21.27 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203154937, tok/s= 88.53, tok/s_decode=107.56, ttft=0.3981, mem/s=1328.81 GB/s, peak_mem=21.23 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203162834, tok/s= 46.92, tok/s_decode= 50.90, ttft=0.3321, mem/s= 352.85 GB/s, peak_mem=15.05 GB, model_size= 7.52 GB quant: int8dq, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization int8dq --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8

20241203163122, tok/s= 87.20, tok/s_decode=110.88, ttft=0.4890, mem/s=1308.91 GB/s, peak_mem=21.54 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203163424, tok/s= 90.43, tok/s_decode=110.60, ttft=0.4021, mem/s=1357.31 GB/s, peak_mem=21.86 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203163706, tok/s= 46.94, tok/s_decode= 50.92, ttft=0.3317, mem/s= 353.03 GB/s, peak_mem=15.01 GB, model_size= 7.52 GB quant: int8dq, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization int8dq --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203164353, tok/s=123.57, tok/s_decode=165.74, ttft=0.4103, mem/s= 929.29 GB/s, peak_mem=16.50 GB, model_size= 7.52 GB quant: int8wo, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization int8wo --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203165042, tok/s= 81.03, tok/s_decode= 93.47, ttft=0.3277, mem/s= 438.09 GB/s, peak_mem=13.13 GB, model_size= 5.41 GB quant: int8dq, sparse: semi-structured, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization int8dq --sparsity semi-structured --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203165235, tok/s=102.72, tok/s_decode=138.39, ttft=0.5012, mem/s= 770.90 GB/s, peak_mem=14.82 GB, model_size= 7.51 GB quant: float8dq, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8dq --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8

20241203165847, tok/s= 87.17, tok/s_decode=110.24, ttft=0.4792, mem/s=1308.47 GB/s, peak_mem=21.52 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203170200, tok/s= 90.07, tok/s_decode=110.20, ttft=0.4048, mem/s=1351.95 GB/s, peak_mem=21.86 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203171007, tok/s= 46.93, tok/s_decode= 50.95, ttft=0.3358, mem/s= 352.91 GB/s, peak_mem=15.05 GB, model_size= 7.52 GB quant: int8dq, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization int8dq --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203171700, tok/s=121.46, tok/s_decode=162.08, ttft=0.4119, mem/s= 913.42 GB/s, peak_mem=16.50 GB, model_size= 7.52 GB quant: int8wo, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization int8wo --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203172349, tok/s= 80.61, tok/s_decode= 93.04, ttft=0.3306, mem/s= 435.82 GB/s, peak_mem=13.13 GB, model_size= 5.41 GB quant: int8dq, sparse: semi-structured, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization int8dq --sparsity semi-structured --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203172606, tok/s=105.83, tok/s_decode=143.17, ttft=0.4922, mem/s= 794.26 GB/s, peak_mem=14.82 GB, model_size= 7.51 GB quant: float8dq, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8dq --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203173049, tok/s=116.23, tok/s_decode=153.28, ttft=0.4153, mem/s= 872.66 GB/s, peak_mem=16.45 GB, model_size= 7.51 GB quant: float8wo, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8wo --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241203173408, tok/s= 37.67, tok/s_decode=191.03, ttft=4.2611, mem/s= 159.05 GB/s, peak_mem=11.39 GB, model_size= 4.22 GB quant: int4wo-64, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization int4wo-64 --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8
20241206060403, tok/s=106.24, tok/s_decode=110.96, ttft=0.2037, mem/s=1594.59 GB/s, peak_mem=19.08 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --prefill_size 4096--num_samples 5 --max_new_tokens 512 --batch_size 1 --top_k 200 --temperature 0.8
20241206060837, tok/s=107.06, tok/s_decode=110.91, ttft=0.1641, mem/s=1607.01 GB/s, peak_mem=19.09 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 4096--num_samples 5 --max_new_tokens 512 --batch_size 1 --top_k 200 --temperature 0.8
20241206062335, tok/s=154.96, tok/s_decode=164.08, ttft=0.1825, mem/s=1165.32 GB/s, peak_mem=13.48 GB, model_size= 7.52 GB quant: int8wo, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization int8wo --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 4096--num_samples 5 --max_new_tokens 512 --batch_size 1 --top_k 200 --temperature 0.8
20241206063026, tok/s=123.86, tok/s_decode=139.85, ttft=0.4677, mem/s= 929.57 GB/s, peak_mem=11.90 GB, model_size= 7.51 GB quant: float8dq, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8dq --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --prefill_size 4096--num_samples 5 --max_new_tokens 512 --batch_size 1 --top_k 200 --temperature 0.8
20241206064929, tok/s=107.10, tok/s_decode=110.92, ttft=0.1630, mem/s=1607.59 GB/s, peak_mem=18.73 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 4096--num_samples 5 --max_new_tokens 512 --batch_size 1 --top_k 200 --temperature 0.8
20241206070012, tok/s=119.70, tok/s_decode=124.40, ttft=0.1601, mem/s=1796.62 GB/s, peak_mem=19.08 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 4096--num_samples 5 --max_new_tokens 512 --batch_size 1 --top_k 200 --temperature 0.8
20241206070700, tok/s= 53.03, tok/s_decode= 53.75, ttft=0.1282, mem/s= 398.78 GB/s, peak_mem=12.39 GB, model_size= 7.52 GB quant: int8dq, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization int8dq --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 4096--num_samples 5 --max_new_tokens 512 --batch_size 1 --top_k 200 --temperature 0.8
20241206072249, tok/s=197.73, tok/s_decode=214.78, ttft=0.2047, mem/s= 768.48 GB/s, peak_mem= 8.13 GB, model_size= 3.89 GB quant: sparse-marlin, sparse: semi-structured, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.float16, device: cuda repro: python generate.py --quantization sparse-marlin --sparsity semi-structured --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.float16 --compile --compile_prefill --prefill_size 4096--num_samples 5 --max_new_tokens 512 --batch_size 1 --top_k 200 --temperature 0.8
20241206072424, tok/s=119.83, tok/s_decode=124.50, ttft=0.1593, mem/s=1798.66 GB/s, peak_mem=18.73 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 4096--num_samples 5 --max_new_tokens 512 --batch_size 1 --top_k 200 --temperature 0.8
20241206072614, tok/s=197.88, tok/s_decode=214.77, ttft=0.2027, mem/s= 769.06 GB/s, peak_mem= 8.00 GB, model_size= 3.89 GB quant: sparse-marlin, sparse: semi-structured, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.float16, device: cuda repro: python generate.py --quantization sparse-marlin --sparsity semi-structured --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.float16 --compile --compile_prefill --prefill_size 4096--num_samples 5 --max_new_tokens 512 --batch_size 1 --top_k 200 --temperature 0.8

20241206072751, tok/s=119.87, tok/s_decode=124.55, ttft=0.1596, mem/s=1799.20 GB/s, peak_mem=18.73 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 4096--num_samples 5 --max_new_tokens 512 --batch_size 1 --top_k 200 --temperature 0.8
20241206072940, tok/s=198.12, tok/s_decode=214.95, ttft=0.2016, mem/s= 770.02 GB/s, peak_mem= 8.00 GB, model_size= 3.89 GB quant: sparse-marlin, sparse: semi-structured, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.float16, device: cuda repro: python generate.py --quantization sparse-marlin --sparsity semi-structured --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.float16 --compile --compile_prefill --prefill_size 4096--num_samples 5 --max_new_tokens 512 --batch_size 1 --top_k 200 --temperature 0.8
20241206074038, tok/s=100.72, tok/s_decode=103.31, ttft=0.1265, mem/s= 544.55 GB/s, peak_mem=10.47 GB, model_size= 5.41 GB quant: int8dq, sparse: semi-structured, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization int8dq --sparsity semi-structured --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 4096--num_samples 5 --max_new_tokens 512 --batch_size 1 --top_k 200 --temperature 0.8

20241206080802, tok/s=198.04, tok/s_decode=214.82, ttft=0.2012, mem/s= 769.68 GB/s, peak_mem= 8.00 GB, model_size= 3.89 GB quant: sparse-marlin-optim, sparse: semi-structured, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.float16, device: cuda repro: python generate.py --quantization sparse-marlin-optim --sparsity semi-structured --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.float16 --compile --compile_prefill --prefill_size 4096--num_samples 5 --max_new_tokens 512 --batch_size 1 --top_k 200 --temperature 0.8
20241206083611, tok/s=  3.58, tok/s_decode=9378.65, ttft=0.4754, mem/s=  19.44 GB/s, peak_mem=16.97 GB, model_size= 5.43 GB quant: sparse-marlin-optim, sparse: semi-structured, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: False, compile_prefill: False, dtype: torch.float16, device: cuda repro: python generate.py --quantization sparse-marlin-optim --sparsity semi-structured --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.float16 --prefill_size 4--num_samples 5 --max_new_tokens 1 --batch_size 1 --top_k 200 --temperature 0.8
20241206083705, tok/s=  3.55, tok/s_decode=6839.86, ttft=0.4799, mem/s=  19.24 GB/s, peak_mem=16.97 GB, model_size= 5.43 GB quant: sparse-marlin-optim, sparse: semi-structured, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: False, compile_prefill: False, dtype: torch.float16, device: cuda repro: python generate.py --quantization sparse-marlin-optim --sparsity semi-structured --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.float16 --prefill_size 4--num_samples 5 --max_new_tokens 1 --batch_size 1 --top_k 200 --temperature 0.8
20241206083956, tok/s=  0.07, tok/s_decode=5417.30, ttft=15.3497, mem/s=   0.52 GB/s, peak_mem=16.41 GB, model_size= 8.04 GB quant: sparse-marlin-optim, sparse: semi-structured, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: False, compile_prefill: False, dtype: torch.float16, device: cuda repro: python generate.py --quantization sparse-marlin-optim --sparsity semi-structured --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.float16 --prefill_size 4--num_samples 5 --max_new_tokens 1 --batch_size 1 --top_k 200 --temperature 0.8

20241206073539, tok/s=185.51, tok/s_decode=198.25, ttft=0.1766, mem/s=1395.06 GB/s, peak_mem=13.45 GB, model_size= 7.52 GB quant: int8wo, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization int8wo --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 4096--num_samples 5 --max_new_tokens 512 --batch_size 1 --top_k 200 --temperature 0.8
20241206085355, tok/s=181.67, tok/s_decode=191.31, ttft=0.1410, mem/s=1461.43 GB/s, peak_mem=12.86 GB, model_size= 8.04 GB quant: sparse-marlin-optim, sparse: semi-structured, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.float16, device: cuda repro: python generate.py --quantization sparse-marlin-optim --sparsity semi-structured --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.float16 --compile --compile_prefill --prefill_size 4096--num_samples 5 --max_new_tokens 512 --batch_size 1 --top_k 200 --temperature 0.8

20241206085747, tok/s=185.94, tok/s_decode=198.58, ttft=0.1746, mem/s=1398.28 GB/s, peak_mem=13.31 GB, model_size= 7.52 GB quant: int8wo, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization int8wo --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 4096--num_samples 5 --max_new_tokens 512 --batch_size 1 --top_k 200 --temperature 0.8
20241206090133, tok/s=181.63, tok/s_decode=190.69, ttft=0.1329, mem/s=1461.08 GB/s, peak_mem=12.25 GB, model_size= 8.04 GB quant: sparse-marlin-optim, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization sparse-marlin-optim --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 4096--num_samples 5 --max_new_tokens 512 --batch_size 1 --top_k 200 --temperature 0.8
20241206090354, tok/s=185.93, tok/s_decode=198.60, ttft=0.1749, mem/s=1398.24 GB/s, peak_mem=13.31 GB, model_size= 7.52 GB quant: int8wo, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization int8wo --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 4096--num_samples 5 --max_new_tokens 512 --batch_size 1 --top_k 200 --temperature 0.8
20241206090639, tok/s=188.65, tok/s_decode=198.32, ttft=0.1314, mem/s=1418.71 GB/s, peak_mem=12.25 GB, model_size= 7.52 GB quant: sparse-marlin-optim, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization sparse-marlin-optim --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 4096--num_samples 5 --max_new_tokens 512 --batch_size 1 --top_k 200 --temperature 0.8

20241206092357, tok/s=197.85, tok/s_decode=214.69, ttft=0.2020, mem/s= 768.95 GB/s, peak_mem= 8.00 GB, model_size= 3.89 GB quant: sparse-marlin-optim, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.float16, device: cuda repro: python generate.py --quantization sparse-marlin-optim --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.float16 --compile --compile_prefill --prefill_size 4096--num_samples 5 --max_new_tokens 512 --batch_size 1 --top_k 200 --temperature 0.8
20241206094301, tok/s=203.31, tok/s_decode=214.73, ttft=0.1332, mem/s=1677.04 GB/s, peak_mem=12.57 GB, model_size= 8.25 GB quant: sparse-marlin-optim, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.float16, device: cuda repro: python generate.py --quantization sparse-marlin-optim --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.float16 --compile --compile_prefill --prefill_size 4096--num_samples 5 --max_new_tokens 512 --batch_size 1 --top_k 200 --temperature 0.8

20241206095209, tok/s=203.54, tok/s_decode=214.82, time=2.5155, ttft=0.1314, mem/s=1678.90 GB/s, peak_mem=12.50 GB, model_size= 8.25 GB quant: sparse-marlin-optim, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.float16, device: cuda repro: python generate.py --quantization sparse-marlin-optim --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.float16 --compile --compile_prefill --prefill_size 4096--num_samples 5 --max_new_tokens 512 --batch_size 1 --top_k 200 --temperature 0.8
20241206095553, tok/s=203.62, tok/s_decode=214.88, time=2.5145, ttft=0.1311, mem/s=1679.55 GB/s, peak_mem=12.50 GB, model_size= 8.25 GB quant: sparse-marlin-optim, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.float16, device: cuda repro: python generate.py --quantization sparse-marlin-optim --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.float16 --compile --compile_prefill --prefill_size 4096--num_samples 5 --max_new_tokens 512 --batch_size 1 --top_k 200 --temperature 0.8
20241206095935, tok/s=203.82, tok/s_decode=215.14, time=2.5120, ttft=0.1314, mem/s=1681.27 GB/s, peak_mem=12.50 GB, model_size= 8.25 GB quant: sparse-marlin-optim, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.float16, device: cuda repro: python generate.py --quantization sparse-marlin-optim --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.float16 --compile --compile_prefill --prefill_size 4096--num_samples 5 --max_new_tokens 512 --batch_size 1 --top_k 200 --temperature 0.8

20241206100405, tok/s=119.86, tok/s_decode=124.48, time=4.2718, ttft=0.1580, mem/s=1799.03 GB/s, peak_mem=19.08 GB, model_size=15.01 GB quant: sparse-marlin, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.float16, device: cuda repro: python generate.py --quantization sparse-marlin --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.float16 --compile --compile_prefill --prefill_size 4096--num_samples 5 --max_new_tokens 512 --batch_size 1 --top_k 200 --temperature 0.8
20241206100749, tok/s=203.89, tok/s_decode=215.18, time=2.5112, ttft=0.1310, mem/s=1681.82 GB/s, peak_mem=12.50 GB, model_size= 8.25 GB quant: sparse-marlin-optim, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.float16, device: cuda repro: python generate.py --quantization sparse-marlin-optim --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.float16 --compile --compile_prefill --prefill_size 4096--num_samples 5 --max_new_tokens 512 --batch_size 1 --top_k 200 --temperature 0.8
20241206101309, tok/s=104.42, tok/s_decode=113.61, time=4.9032, ttft=0.3958, mem/s=1567.34 GB/s, peak_mem=21.50 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 512 --batch_size 1 --top_k 200 --temperature 0.8

20241206102237, tok/s=104.47, tok/s_decode=113.58, time=4.9010, ttft=0.3923, mem/s=1568.06 GB/s, peak_mem=21.34 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 512 --batch_size 1 --top_k 200 --temperature 0.8
20241206111713, tok/s=166.38, tok/s_decode=184.89, time=3.0773, ttft=0.3063, mem/s=1372.41 GB/s, peak_mem=15.34 GB, model_size= 8.25 GB quant: sparse-marlin-optim, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.float16, device: cuda repro: python generate.py --quantization sparse-marlin-optim --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.float16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 512 --batch_size 1 --top_k 200 --temperature 0.8
20241206103003, tok/s=157.47, tok/s_decode=184.48, time=3.2513, ttft=0.4751, mem/s= 612.03 GB/s, peak_mem=10.86 GB, model_size= 3.89 GB quant: sparse-marlin, sparse: semi-structured, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.float16, device: cuda repro: python generate.py --quantization sparse-marlin --sparsity semi-structured --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.float16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 512 --batch_size 1 --top_k 200 --temperature 0.8

20241206112249, tok/s= 49.99, tok/s_decode= 51.64, time=10.2413, ttft=0.3266, mem/s= 375.96 GB/s, peak_mem=15.20 GB, model_size= 7.52 GB quant: int8dq-optim, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization int8dq-optim --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 512 --batch_size 1 --top_k 200 --temperature 0.8
20241206112905, tok/s=166.58, tok/s_decode=185.10, time=3.0735, ttft=0.3065, mem/s=1374.09 GB/s, peak_mem=15.31 GB, model_size= 8.25 GB quant: sparse-marlin-optim, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.float16, device: cuda repro: python generate.py --quantization sparse-marlin-optim --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.float16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 512 --batch_size 1 --top_k 200 --temperature 0.8
20241206113213, tok/s=157.33, tok/s_decode=174.88, time=3.2544, ttft=0.3258, mem/s=1183.14 GB/s, peak_mem=15.10 GB, model_size= 7.52 GB quant: int8dq-optim, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: True, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization int8dq-optim --checkpoint_path ../../../checkpoints/meta-llama/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --compile_prefill --prefill_size 8192--num_samples 5 --max_new_tokens 512 --batch_size 1 --top_k 200 --temperature 0.8
