name: A100-perf-nightly

on:
  schedule:
    - cron: 0 7 * * 0-6

jobs:
  benchmark:
    runs-on: linux.aws.a100
    
    secrets:
      HUGGING_FACE_HUB_TOKEN: ${{ secrets.HUGGING_FACE_HUB_TOKEN }}
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

    strategy:
      matrix:
        torch-spec: '--pre torch --index-url https://download.pytorch.org/whl/nightly/cu124'

    steps:
      - uses: actions/checkout@v3

      - name: Run your own benchmark logic
        shell: bash
        run: |
          set -eux
          conda create -n venv python=3.9 -y
          conda activate venv
          python -m pip install --upgrade pip
          pip install ${{ matrix.torch-spec }}
          pip install -r dev-requirements.txt
          pip install .
          export CONDA=$(dirname $(dirname $(which conda)))
          export LD_LIBRARY_PATH=$CONDA/lib/:$LD_LIBRARY_PATH
          cd torchao/_models/llama
          export CHECKPOINT_PATH=../../../checkpoints # path to checkpoints folder
          export MODEL_REPO=meta-llama/Llama-2-7b-chat-hf
          python scripts/download.py --repo_id meta-llama/Llama-2-7b-chat-hf --hf_token ${{ secrets.HUGGING_FACE_HUB_TOKEN }}
          python scripts/convert_hf_checkpoint.py
          python generate.py --checkpoint_path $CHECKPOINT_PATH/$MODEL_REPO/model.pth --compile --output_json_path ${{ runner.temp }}/benchmark-results/benchmark-results.json


     - name: Upload the benchmark results to OSS benchmark database for the dashboard
       uses: pytorch/test-infra/.github/actions/upload-benchmark-results@main
       with:
         benchmark-results-dir: ${{ runner.temp }}/benchmark-results
         dry-run: false
         schema-version: v3
         github-token: ${{ secrets.GITHUB_TOKEN }}

  
