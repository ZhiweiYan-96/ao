


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torchao.dtypes.affine_quantized_tensor &mdash; torchao main documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sphinx-design.5ea377869091fd0449014c60fc090103.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','');</script>
    <!-- End Google Tag Manager -->
  

  
  <script src="../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2023">
                  <span class="dropdown-title">Contributor Awards - 2023</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News 
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                  <p></p>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
        <a href='tba'>main &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api_ref_sparsity.html">torchao.sparsity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_ref_intro.html"><code class="docutils literal notranslate"><span class="pre">torchao</span></code> API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_ref_quantization.html">torchao.quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_ref_dtypes.html">torchao.dtypes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../serialization.html">Serialization</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
      <li>torchao.dtypes.affine_quantized_tensor</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id="
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torchao.dtypes.affine_quantized_tensor</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Union</span><span class="p">,</span> <span class="n">List</span>
<span class="kn">import</span> <span class="nn">torchao.ops</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">from</span> <span class="nn">torchao.quantization.quant_primitives</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_get_reduction_params</span><span class="p">,</span>
    <span class="n">choose_qparams_affine</span><span class="p">,</span>
    <span class="n">quantize_affine</span><span class="p">,</span>
    <span class="n">dequantize_affine</span><span class="p">,</span>
    <span class="n">ZeroPointDomain</span><span class="p">,</span>
    <span class="n">MappingType</span><span class="p">,</span>
    <span class="n">int_scaled_matmul</span><span class="p">,</span>
    <span class="n">choose_qparams_and_quantize_affine_hqq</span><span class="p">,</span>
    <span class="n">FP8_TYPES</span><span class="p">,</span>
    <span class="n">choose_qparams_affine_floatx</span><span class="p">,</span>
    <span class="n">quantize_affine_floatx</span><span class="p">,</span>
    <span class="n">dequantize_affine_floatx</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torchao.quantization.utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">pack_tinygemm_scales_and_zeros</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torch.utils._python_dispatch</span> <span class="kn">import</span> <span class="n">return_and_correct_aliasing</span>
<span class="kn">from</span> <span class="nn">torchao.dtypes.utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">Layout</span><span class="p">,</span>
    <span class="n">PlainLayout</span><span class="p">,</span>
    <span class="n">is_device</span><span class="p">,</span>
    <span class="n">get_out_shape</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torchao.float8.inference</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">preprocess_data</span><span class="p">,</span>
    <span class="n">Float8MMConfig</span><span class="p">,</span>
    <span class="n">addmm_float8_unwrapped_inference</span><span class="p">,</span>
    <span class="n">_is_rowwise_scaled</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torch.utils._python_dispatch</span> <span class="kn">import</span> <span class="n">is_traceable_wrapper_subclass</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">torchao.utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">find_multiple</span><span class="p">,</span>
    <span class="n">TorchAOBaseTensor</span><span class="p">,</span>
    <span class="n">TORCH_VERSION_AT_LEAST_2_5</span><span class="p">,</span>
    <span class="n">_is_float8_type</span><span class="p">,</span>
    <span class="n">fill_defaults</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">import</span> <span class="nn">logging</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">torchao.float8.inference</span> <span class="kn">import</span> <span class="n">Float8MMConfig</span>
<span class="n">aten</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span>

<span class="c1">###############################</span>
<span class="c1"># Base Tensor Impl Subclass #</span>
<span class="c1">###############################</span>
<span class="k">class</span> <span class="nc">AQTTensorImpl</span><span class="p">(</span><span class="n">TorchAOBaseTensor</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Base class for the tensor impl for `AffineQuantizedTensor`</span>

<span class="sd">    Note: This is not a user facing API, it&#39;s used by AffineQuantizedTensor to construct</span>
<span class="sd">    the underlying implementation of a AQT based on layout</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">get_plain</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the plain (unpacked) Tensor for the tensor impl</span>

<span class="sd">        Returns data, scale and zero_point</span>
<span class="sd">        Can be overwritten if other types of AQTTensorImpl has different numbers of plain tensors</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">get_layout</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Layout</span><span class="p">:</span>
        <span class="k">pass</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_plain</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">data</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">zero_point</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">_layout</span><span class="p">:</span> <span class="n">Layout</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Construct a TensorImpl from data, scale, zero_point and the _layout&quot;&quot;&quot;</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">data</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_plain</span><span class="p">()</span>
        <span class="n">_layout</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_layout</span><span class="p">()</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">(data=</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="si">}</span><span class="s2">... , scale=</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">scale</span><span class="p">)</span><span class="si">}</span><span class="s2">... , zero_point=</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">zero_point</span><span class="p">)</span><span class="si">}</span><span class="s2">... , _layout=</span><span class="si">{</span><span class="n">_layout</span><span class="si">}</span><span class="s2">)&quot;</span>


<span class="c1">##############################</span>
<span class="c1"># Tensor Subclass Definition #</span>
<span class="c1">##############################</span>


<span class="k">class</span> <span class="nc">QuantizedLinearNotImplementedError</span><span class="p">(</span><span class="ne">NotImplementedError</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Thin wrapper around NotImplementedError to make it easier to catch this error in the dispatch table &quot;&quot;&quot;</span>
    <span class="k">pass</span>


<span class="n">_AQT_QLINEAR_DISPATCH_TABLE</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">def</span> <span class="nf">register_aqt_quantized_linear_dispatch</span><span class="p">(</span><span class="n">dispatch_condition</span><span class="p">,</span> <span class="n">impl</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Register a dispatch for quantized linear op with dispatch_condition function and impl function</span>
<span class="sd">    both takes three arguments:</span>
<span class="sd">      input_tensor: dimension is (M1, M2, ..., in_features)</span>
<span class="sd">      weight_tensor: dimension is (out_features, in_features)</span>
<span class="sd">      bias: dimension is (out_features,)</span>
<span class="sd">      so that these can be shared by F.linear, aten.mm, aten.addmm dispatches</span>

<span class="sd">    Args:</span>
<span class="sd">        `dispatch_condition` (Callable[[torch.Tensor, torch.Tensor, torch.Tensor], bool]: the dispatch</span>
<span class="sd">            condition for a specialized quantized linear implementation, e.g. bfloat16 activation + uint4 weight</span>
<span class="sd">        `impl` (Callable[[torch.Tensor, torch.Tensor, torch.Tensor], torch.Tensor]: the specialized</span>
<span class="sd">            quantized linear implementation</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_AQT_QLINEAR_DISPATCH_TABLE</span><span class="p">[</span><span class="n">dispatch_condition</span><span class="p">]</span> <span class="o">=</span> <span class="n">impl</span>

<span class="k">def</span> <span class="nf">deregister_aqt_quantized_linear_dispatch</span><span class="p">(</span><span class="n">dispatch_condition</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">dispatch_condition</span> <span class="ow">in</span> <span class="n">_AQT_QLINEAR_DISPATCH_TABLE</span><span class="p">:</span>
        <span class="k">del</span> <span class="n">_AQT_QLINEAR_DISPATCH_TABLE</span><span class="p">[</span><span class="n">dispatch_condition</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Attempting to remove non-existant dispatch condition </span><span class="si">{</span><span class="n">dispatch_condition</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<div class="viewcode-block" id="AffineQuantizedTensor"><a class="viewcode-back" href="../../../generated/torchao.dtypes.AffineQuantizedTensor.html#torchao.dtypes.AffineQuantizedTensor">[docs]</a><span class="k">class</span> <span class="nc">AffineQuantizedTensor</span><span class="p">(</span><span class="n">TorchAOBaseTensor</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Affine quantized tensor subclass. Affine quantization means we quantize the floating point tensor with an affine transformation:</span>
<span class="sd">       quantized_tensor = float_tensor / scale + zero_point</span>

<span class="sd">    To see what happens during choose_qparams, quantization and dequantization for affine quantization,</span>
<span class="sd">    please checkout https://github.com/pytorch/ao/blob/main/torchao/quantization/quant_primitives.py</span>
<span class="sd">    and check the three quant primitive ops: choose_qparams_affine, quantize_affine qand dequantize_affine</span>

<span class="sd">    The shape and dtype of the tensor subclass represent how the tensor subclass looks externally,</span>
<span class="sd">    regardless of the internal representation&#39;s type or orientation.</span>

<span class="sd">    fields:</span>
<span class="sd">      tensor_impl (AQTTensorImpl): tensor that serves as a general tensor impl storage for the quantized data,</span>
<span class="sd">         e.g. storing plain tensors (int_data, scale, zero_point) or packed formats depending on device</span>
<span class="sd">         and operator/kernel</span>
<span class="sd">      block_size (Tuple[int, ...]): granularity of quantization, this means the size of the tensor elements that&#39;s sharing the same qparam</span>
<span class="sd">         e.g. when size is the same as the input tensor dimension, we are using per tensor quantization</span>
<span class="sd">      shape (torch.Size): the shape for the original high precision Tensor</span>
<span class="sd">      quant_min (Optional[int]): minimum quantized value for the Tensor, if not specified, it will be derived from dtype of `int_data`</span>
<span class="sd">      quant_max (Optional[int]): maximum quantized value for the Tensor, if not specified, it will be derived from dtype of `int_data`</span>
<span class="sd">      zero_point_domain (ZeroPointDomain): the domain that zero_point is in, should be either integer or float</span>
<span class="sd">        if zero_point is in integer domain, zero point is added to the quantized integer value during</span>
<span class="sd">        quantization</span>
<span class="sd">        if zero_point is in floating point domain, zero point is subtracted from the floating point (unquantized)</span>
<span class="sd">        value during quantization</span>
<span class="sd">        default is ZeroPointDomain.INT</span>
<span class="sd">      dtype: dtype for original high precision tensor, e.g. torch.float32</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="fm">__new__</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">tensor_impl</span><span class="p">:</span> <span class="n">AQTTensorImpl</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">shape</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">zero_point_domain</span><span class="p">:</span> <span class="n">ZeroPointDomain</span> <span class="o">=</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">INT</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">strides</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;device&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tensor_impl</span><span class="o">.</span><span class="n">device</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;layout&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;layout&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;layout&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="k">else</span> <span class="n">tensor_impl</span><span class="o">.</span><span class="n">layout</span>
        <span class="p">)</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;dtype&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">dtype</span>
        <span class="k">if</span> <span class="n">strides</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;strides&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">strides</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;requires_grad&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_make_wrapper_subclass</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">tensor_impl</span><span class="p">:</span> <span class="n">AQTTensorImpl</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">shape</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">zero_point_domain</span><span class="p">:</span> <span class="n">ZeroPointDomain</span> <span class="o">=</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">INT</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">strides</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tensor_impl</span> <span class="o">=</span> <span class="n">tensor_impl</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">=</span> <span class="n">block_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">quant_min</span> <span class="o">=</span> <span class="n">quant_min</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">quant_max</span> <span class="o">=</span> <span class="n">quant_max</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">zero_point_domain</span> <span class="o">=</span> <span class="n">zero_point_domain</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">(tensor_impl=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">tensor_impl</span><span class="si">}</span><span class="s2">, block_size=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">block_size</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;shape=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, device=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2">, dtype=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">, requires_grad=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="si">}</span><span class="s2">)&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_quantization_type</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;shape=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, block_size=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">block_size</span><span class="si">}</span><span class="s2">, device=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2">, _layout=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="si">}</span><span class="s2">, tensor_impl_dtype=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">, quant_min=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">quant_min</span><span class="si">}</span><span class="s2">, quant_max=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">quant_max</span><span class="si">}</span><span class="s2">&quot;</span>

<div class="viewcode-block" id="AffineQuantizedTensor.dequantize"><a class="viewcode-back" href="../../../generated/torchao.dtypes.AffineQuantizedTensor.html#torchao.dtypes.AffineQuantizedTensor.dequantize">[docs]</a>    <span class="k">def</span> <span class="nf">dequantize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">output_dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">output_dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span>

        <span class="kn">from</span> <span class="nn">torchao.dtypes.floatx</span> <span class="kn">import</span> <span class="n">FloatxTensorCoreLayout</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="p">,</span> <span class="n">FloatxTensorCoreLayout</span><span class="p">):</span>
            <span class="n">int_data</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">get_plain</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">dequantize_affine_floatx</span><span class="p">(</span><span class="n">int_data</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="o">.</span><span class="n">ebits</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="o">.</span><span class="n">mbits</span><span class="p">,</span> <span class="n">output_dtype</span><span class="o">=</span><span class="n">output_dtype</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">data</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">get_plain</span><span class="p">()</span>
            <span class="n">dq</span> <span class="o">=</span> <span class="n">dequantize_affine</span><span class="p">(</span>
                <span class="n">data</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span>
                <span class="n">scale</span><span class="p">,</span>
                <span class="n">zero_point</span><span class="p">,</span>
                <span class="n">data</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">quant_min</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">quant_max</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">zero_point_domain</span><span class="p">,</span>
                <span class="n">output_dtype</span><span class="o">=</span><span class="n">output_dtype</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="c1"># need to return to original shape if tensor was padded</span>
            <span class="c1"># in preprocessing</span>
            <span class="k">for</span> <span class="n">dim</span><span class="p">,</span> <span class="n">dim_size</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">):</span>
                <span class="n">dq</span> <span class="o">=</span> <span class="n">dq</span><span class="o">.</span><span class="n">narrow</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">dq</span></div>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_quantized_linear_op</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">weight_tensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">dispatch_condition</span><span class="p">,</span> <span class="n">impl</span> <span class="ow">in</span> <span class="n">_AQT_QLINEAR_DISPATCH_TABLE</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">dispatch_condition</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">weight_tensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">impl</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">weight_tensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
        <span class="k">raise</span> <span class="n">QuantizedLinearNotImplementedError</span><span class="p">(</span><span class="s2">&quot;No specialized dispatch found for quantized linear op&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__tensor_flatten__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="s2">&quot;tensor_impl&quot;</span><span class="p">],</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">quant_min</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">quant_max</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero_point_domain</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">__tensor_unflatten__</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span> <span class="n">tensor_data_dict</span><span class="p">,</span> <span class="n">tensor_attributes</span><span class="p">,</span> <span class="n">outer_size</span><span class="p">,</span> <span class="n">outer_stride</span>
    <span class="p">):</span>
        <span class="n">tensor_impl</span> <span class="o">=</span> <span class="n">tensor_data_dict</span><span class="p">[</span><span class="s2">&quot;tensor_impl&quot;</span><span class="p">]</span>
        <span class="n">block_size</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">,</span> <span class="n">zero_point_domain</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">tensor_attributes</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span>
            <span class="n">tensor_impl</span><span class="p">,</span>
            <span class="n">block_size</span><span class="p">,</span>
            <span class="n">shape</span> <span class="k">if</span> <span class="n">outer_size</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">outer_size</span><span class="p">,</span>
            <span class="n">quant_min</span><span class="p">,</span>
            <span class="n">quant_max</span><span class="p">,</span>
            <span class="n">zero_point_domain</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">strides</span><span class="o">=</span><span class="n">outer_stride</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_hp_to_intx</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">input_float</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">mapping_type</span><span class="p">:</span> <span class="n">MappingType</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">target_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">eps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">scale_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">zero_point_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">preserve_zero</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">zero_point_domain</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ZeroPointDomain</span><span class="p">]</span> <span class="o">=</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">INT</span><span class="p">,</span>
        <span class="n">_layout</span><span class="p">:</span> <span class="n">Layout</span> <span class="o">=</span> <span class="n">PlainLayout</span><span class="p">(),</span>
        <span class="n">use_hqq</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">original_shape</span> <span class="o">=</span> <span class="n">input_float</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">input_float</span> <span class="o">=</span> <span class="n">_layout</span><span class="o">.</span><span class="n">pre_process</span><span class="p">(</span><span class="n">input_float</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">use_hqq</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">zero_point_domain</span> <span class="o">==</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">FLOAT</span> <span class="ow">and</span> <span class="n">mapping_type</span> <span class="o">==</span> <span class="n">MappingType</span><span class="o">.</span><span class="n">ASYMMETRIC</span> <span class="ow">and</span> <span class="n">quant_min</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;Invalid input parameters for HQQ quantization.&quot;</span>
            <span class="n">nbits</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">quant_max</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
            <span class="n">axis</span>  <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="p">(</span><span class="n">block_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">==</span><span class="mi">1</span><span class="p">)</span> <span class="k">else</span> <span class="mi">0</span>
            <span class="n">group_size</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">block_size</span><span class="p">)</span>
            <span class="n">compute_dtype</span> <span class="o">=</span> <span class="n">zero_point_dtype</span> <span class="k">if</span> <span class="p">(</span><span class="n">zero_point_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="k">else</span> <span class="n">input_float</span><span class="o">.</span><span class="n">dtype</span>
            <span class="n">device</span> <span class="o">=</span> <span class="n">input_float</span><span class="o">.</span><span class="n">device</span>
            <span class="n">data</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">choose_qparams_and_quantize_affine_hqq</span><span class="p">(</span><span class="n">input_float</span><span class="p">,</span> <span class="n">nbits</span><span class="o">=</span><span class="n">nbits</span><span class="p">,</span> <span class="n">group_size</span><span class="o">=</span><span class="n">group_size</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">compute_dtype</span><span class="o">=</span><span class="n">compute_dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">raw_output</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">target_dtype</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span> <span class="o">=</span> <span class="n">choose_qparams_affine</span><span class="p">(</span><span class="n">input_float</span><span class="p">,</span> <span class="n">mapping_type</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">target_dtype</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">scale_dtype</span><span class="p">,</span> <span class="n">zero_point_dtype</span><span class="p">,</span> <span class="n">preserve_zero</span><span class="p">,</span> <span class="n">zero_point_domain</span><span class="p">)</span>
            <span class="c1"># choose_qparams_affine is a custom op that does support returning optional Tensors. We thus set the zero_point to None if its domain is None</span>
            <span class="k">if</span> <span class="n">zero_point_domain</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">zero_point</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">quantize_affine</span><span class="p">(</span><span class="n">input_float</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span><span class="p">,</span> <span class="n">target_dtype</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">,</span> <span class="n">zero_point_domain</span><span class="p">)</span>
            <span class="c1"># Note: output will be uint8 tensor for sub byte tensors for now</span>

        <span class="n">data</span> <span class="o">=</span> <span class="n">_layout</span><span class="o">.</span><span class="n">post_process</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">tensor_impl_ctr</span> <span class="o">=</span> <span class="n">get_tensor_impl_constructor</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">_layout</span><span class="p">))</span>
        <span class="n">tensor_impl</span> <span class="o">=</span> <span class="n">tensor_impl_ctr</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span><span class="p">,</span> <span class="n">_layout</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span>
            <span class="n">tensor_impl</span><span class="p">,</span>
            <span class="n">block_size</span><span class="p">,</span>
            <span class="n">original_shape</span><span class="p">,</span>
            <span class="n">quant_min</span><span class="p">,</span>
            <span class="n">quant_max</span><span class="p">,</span>
            <span class="n">zero_point_domain</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">input_float</span><span class="o">.</span><span class="n">dtype</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_hp_to_intx_static</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">input_float</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">block_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">target_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">zero_point_domain</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ZeroPointDomain</span><span class="p">]</span> <span class="o">=</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">INT</span><span class="p">,</span>
        <span class="n">_layout</span><span class="p">:</span> <span class="n">Layout</span> <span class="o">=</span> <span class="n">PlainLayout</span><span class="p">(),</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">target_dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">FP8_TYPES</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">zero_point_domain</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;zero_point_domain must be specified for non-fp8 types&quot;</span>
            <span class="k">assert</span> <span class="n">zero_point</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;zero_point must be specified for non-fp8 types&quot;</span>
        <span class="n">original_shape</span> <span class="o">=</span> <span class="n">input_float</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">input_float</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span> <span class="o">=</span> <span class="n">_layout</span><span class="o">.</span><span class="n">pre_process_static</span><span class="p">(</span><span class="n">input_float</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span><span class="p">,</span> <span class="n">block_size</span><span class="p">)</span>

        <span class="n">int_data</span> <span class="o">=</span> <span class="n">quantize_affine</span><span class="p">(</span><span class="n">input_float</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span><span class="p">,</span> <span class="n">target_dtype</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">,</span> <span class="n">zero_point_domain</span><span class="p">)</span>

        <span class="n">int_data</span> <span class="o">=</span> <span class="n">_layout</span><span class="o">.</span><span class="n">post_process</span><span class="p">(</span><span class="n">int_data</span><span class="p">)</span>

        <span class="n">tensor_impl_ctr</span> <span class="o">=</span> <span class="n">get_tensor_impl_constructor</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">_layout</span><span class="p">))</span>
        <span class="n">tensor_impl</span> <span class="o">=</span> <span class="n">tensor_impl_ctr</span><span class="p">(</span><span class="n">int_data</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span><span class="p">,</span> <span class="n">_layout</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span>
            <span class="n">tensor_impl</span><span class="p">,</span>
            <span class="n">block_size</span><span class="p">,</span>
            <span class="n">original_shape</span><span class="p">,</span>
            <span class="n">quant_min</span><span class="p">,</span>
            <span class="n">quant_max</span><span class="p">,</span>
            <span class="n">zero_point_domain</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">input_float</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_hp_to_floatx</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">input_float</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">target_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">_layout</span><span class="p">:</span> <span class="n">Layout</span><span class="p">,</span>
        <span class="n">scale_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>

        <span class="k">if</span> <span class="n">target_dtype</span> <span class="ow">in</span> <span class="n">FP8_TYPES</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">from_hp_to_intx</span><span class="p">(</span>
                <span class="n">input_float</span><span class="o">=</span><span class="n">input_float</span><span class="p">,</span>
                <span class="n">mapping_type</span><span class="o">=</span><span class="n">MappingType</span><span class="o">.</span><span class="n">SYMMETRIC</span><span class="p">,</span>
                <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span>
                <span class="n">target_dtype</span><span class="o">=</span><span class="n">target_dtype</span><span class="p">,</span>
                <span class="n">quant_min</span><span class="o">=</span><span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">target_dtype</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">),</span>
                <span class="n">quant_max</span><span class="o">=</span><span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">target_dtype</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">),</span>
                <span class="n">eps</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span>
                <span class="n">scale_dtype</span><span class="o">=</span><span class="n">scale_dtype</span><span class="p">,</span>
                <span class="n">zero_point_dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">preserve_zero</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">zero_point_domain</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">_layout</span><span class="o">=</span><span class="n">_layout</span><span class="p">,</span>
                <span class="n">use_hqq</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unsupported dtype </span><span class="si">{</span><span class="n">target_dtype</span><span class="si">}</span><span class="s2"> for from_hp_to_floatx&quot;</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_hp_to_floatx_static</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">input_float</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">target_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">_layout</span><span class="p">:</span> <span class="n">Layout</span><span class="p">,</span>
    <span class="p">):</span>

        <span class="k">if</span> <span class="n">target_dtype</span> <span class="ow">in</span> <span class="n">FP8_TYPES</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">from_hp_to_intx_static</span><span class="p">(</span>
                <span class="n">input_float</span><span class="o">=</span><span class="n">input_float</span><span class="p">,</span>
                <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span>
                <span class="n">zero_point</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span>
                <span class="n">target_dtype</span><span class="o">=</span><span class="n">target_dtype</span><span class="p">,</span>
                <span class="n">quant_min</span><span class="o">=</span><span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">target_dtype</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">),</span>
                <span class="n">quant_max</span><span class="o">=</span><span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">target_dtype</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">),</span>
                <span class="n">zero_point_domain</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">_layout</span><span class="o">=</span><span class="n">_layout</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unsupported dtype </span><span class="si">{</span><span class="n">target_dtype</span><span class="si">}</span><span class="s2"> for from_hp_to_floatx_static&quot;</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_hp_to_fpx</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">input_float</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">_layout</span><span class="p">:</span> <span class="n">Layout</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="kn">from</span> <span class="nn">torchao.dtypes.floatx</span> <span class="kn">import</span> <span class="n">FloatxTensorCoreLayout</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">_layout</span><span class="p">,</span> <span class="n">FloatxTensorCoreLayout</span><span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Only FloatxTensorCoreLayout is supported for floatx, got </span><span class="si">{</span><span class="n">_layout</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="n">original_shape</span> <span class="o">=</span> <span class="n">input_float</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">input_float</span> <span class="o">=</span> <span class="n">_layout</span><span class="o">.</span><span class="n">pre_process</span><span class="p">(</span><span class="n">input_float</span><span class="p">)</span>
        <span class="c1"># per axis quantization, where axis = 1</span>
        <span class="n">block_size</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">input_float</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">block_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="n">ebits</span><span class="p">,</span> <span class="n">mbits</span> <span class="o">=</span> <span class="n">_layout</span><span class="o">.</span><span class="n">ebits</span><span class="p">,</span> <span class="n">_layout</span><span class="o">.</span><span class="n">mbits</span>
        <span class="c1"># Note: these ops are hardcoded to have per axis quantization (axis=1) right now</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">choose_qparams_affine_floatx</span><span class="p">(</span><span class="n">input_float</span><span class="p">,</span> <span class="n">ebits</span><span class="p">,</span> <span class="n">mbits</span><span class="p">)</span>
        <span class="n">floatx_unpacked</span> <span class="o">=</span> <span class="n">quantize_affine_floatx</span><span class="p">(</span><span class="n">input_float</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">ebits</span><span class="p">,</span> <span class="n">mbits</span><span class="p">)</span>
        <span class="n">floatx_packed</span> <span class="o">=</span> <span class="n">_layout</span><span class="o">.</span><span class="n">post_process</span><span class="p">(</span><span class="n">floatx_unpacked</span><span class="p">)</span>

        <span class="n">tensor_impl_ctr</span> <span class="o">=</span> <span class="n">get_tensor_impl_constructor</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">_layout</span><span class="p">))</span>
        <span class="n">tensor_impl</span> <span class="o">=</span> <span class="n">tensor_impl_ctr</span><span class="p">(</span><span class="n">floatx_packed</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">_layout</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span>
            <span class="n">tensor_impl</span><span class="p">,</span>
            <span class="n">block_size</span><span class="p">,</span>
            <span class="n">original_shape</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">input_float</span><span class="o">.</span><span class="n">dtype</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_layout</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Layout</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">_layout</span>

<div class="viewcode-block" id="AffineQuantizedTensor.to"><a class="viewcode-back" href="../../../generated/torchao.dtypes.AffineQuantizedTensor.html#torchao.dtypes.AffineQuantizedTensor.to">[docs]</a>    <span class="k">def</span> <span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_to_kwargs</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;device&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">quant_min</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">quant_max</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">zero_point_domain</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_apply_fn_to_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fn</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">(</span>
            <span class="n">fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tensor_impl</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">quant_min</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">quant_max</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">zero_point_domain</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">strides</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span>
        <span class="p">)</span></div>

    <span class="c1"># following are the comments for __torch_function__/__torch_dispatch__, we can clean this up</span>
    <span class="c1"># a bit later</span>
    <span class="c1"># Note: we only added cpu path here for 8da4w, this is for executorch, in the future</span>
    <span class="c1"># 1. we&#39;ll add cpu/cuda version (int4mm etc.)</span>
    <span class="c1"># 2. we&#39;ll need to hide the 8da4w executorch version under things like layouts (we also have multiple impl for cpu kernel as Michael mentioned), so it will be something like</span>
    <span class="c1">#   cpu device + et laytout --&gt; gives current 8da4w executorch representation</span>
    <span class="c1">#   cpu device + avx layout --&gt; gives optimized kernel for 8da4w in avx cpu etc.</span>
    <span class="c1">#   cuda device + some layout --&gt; gives cuda kernel</span>

    <span class="c1"># two scenarios where we currently fall back to vanilla mm:</span>
    <span class="c1"># 1 - when tensor is on CUDA: we&#39;ll add this later, we&#39;ll also enable dispatching to optimized</span>
    <span class="c1">#     kernels in CPU as well, see the note above</span>
    <span class="c1"># 2 - we&#39;re given non-floats - quantizing long to int8 is crazy</span>


<span class="c1">######################################################</span>
<span class="c1"># Layout and TensorImpl Subclass Registration #</span>
<span class="c1">######################################################</span>
<span class="n">register_layout</span> <span class="o">=</span> <span class="n">AffineQuantizedTensor</span><span class="o">.</span><span class="n">register_layout</span>
<span class="n">get_tensor_impl_constructor</span> <span class="o">=</span> <span class="n">AffineQuantizedTensor</span><span class="o">.</span><span class="n">get_tensor_impl_constructor</span>

<span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">SemiSparseLayout</span><span class="p">(</span><span class="n">Layout</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">pre_process</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="c1"># prune to 2:4 if not already</span>
        <span class="n">temp</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="n">pruning_inds</span> <span class="o">=</span> <span class="n">temp</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">temp</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">pruning_inds</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">temp</span>


<span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">BlockSparseLayout</span><span class="p">(</span><span class="n">Layout</span><span class="p">):</span>
    <span class="n">blocksize</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span>


<span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">TensorCoreTiledLayout</span><span class="p">(</span><span class="n">Layout</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    inner_k_tiles is an internal argument for packing function of tensor core tiled layout</span>
<span class="sd">    that can affect the performance of the matmul kernel</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inner_k_tiles</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span>

    <span class="k">def</span> <span class="nf">pre_process</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">orig_out_features</span><span class="p">,</span> <span class="n">orig_in_features</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">in_features</span> <span class="o">=</span> <span class="n">find_multiple</span><span class="p">(</span><span class="n">orig_in_features</span><span class="p">,</span> <span class="mi">1024</span><span class="p">)</span>
        <span class="n">out_features</span> <span class="o">=</span> <span class="n">find_multiple</span><span class="p">(</span><span class="n">orig_out_features</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span>
            <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">in_features</span> <span class="o">-</span> <span class="n">orig_in_features</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">out_features</span> <span class="o">-</span> <span class="n">orig_out_features</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="nb">input</span>

    <span class="k">def</span> <span class="nf">pre_process_static</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">zero_point</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">block_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre_process</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">orig_qparam_shape</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">new_qparam_shape</span><span class="p">,</span> <span class="n">reduction_dims</span> <span class="o">=</span> <span class="n">_get_reduction_params</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
        <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="n">reduction_dims</span><span class="p">:</span>
            <span class="n">new_qparam_shape</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
        <span class="n">change_in_qparam_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">new_dim_size</span><span class="o">-</span><span class="n">orig_dim_size</span> <span class="k">for</span> <span class="n">new_dim_size</span><span class="p">,</span> <span class="n">orig_dim_size</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">new_qparam_shape</span><span class="p">,</span> <span class="n">orig_qparam_shape</span><span class="p">)]</span>
        <span class="n">padding_changes</span><span class="o">=</span><span class="p">[]</span>
        <span class="k">for</span> <span class="n">dim_change</span> <span class="ow">in</span> <span class="n">change_in_qparam_shape</span><span class="p">:</span>
            <span class="n">padding_changes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim_change</span><span class="p">]</span> <span class="o">+</span> <span class="n">padding_changes</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="n">padding_changes</span><span class="p">)</span>
        <span class="n">zero_point</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">zero_point</span><span class="p">,</span> <span class="n">padding_changes</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">input</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span>

    <span class="k">def</span> <span class="nf">post_process</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">orig_out_features</span><span class="p">,</span> <span class="n">orig_in_features</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">in_features</span> <span class="o">=</span> <span class="n">find_multiple</span><span class="p">(</span><span class="n">orig_in_features</span><span class="p">,</span> <span class="mi">1024</span><span class="p">)</span>
        <span class="n">out_features</span> <span class="o">=</span> <span class="n">find_multiple</span><span class="p">(</span><span class="n">orig_out_features</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span>
            <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">in_features</span> <span class="o">-</span> <span class="n">orig_in_features</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">out_features</span> <span class="o">-</span> <span class="n">orig_out_features</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="nb">input</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;inner_k_tiles=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">inner_k_tiles</span><span class="si">}</span><span class="s2">&quot;</span>


<span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">Float8Layout</span><span class="p">(</span><span class="n">Layout</span><span class="p">):</span>
    <span class="n">mm_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Float8MMConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>


<span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">MarlinSparseLayout</span><span class="p">(</span><span class="n">Layout</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">pre_process</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Preprocess the input tensor to be in the correct format for the Marlin sparse kernel.</span>
<span class="sd">            - 1º: the input tensor is transposed since the linear layer keeps the weights in a transposed format</span>
<span class="sd">            - 2º: tensor is injected with 2:4 sparsity</span>
<span class="sd">            - 3º: transposes it again because the quantization process will compute the scales for dim=-1</span>

<span class="sd">        Args:</span>
<span class="sd">            input (torch.Tensor): the input tensor to preprocess</span>

<span class="sd">        Returns:</span>
<span class="sd">            torch.Tensor: the preprocessed tensor</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">torchao.sparsity.marlin</span> <span class="kn">import</span> <span class="n">inject_24</span>  <span class="c1"># avoid circular import</span>
        <span class="n">input_t</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
        <span class="n">w_24</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">inject_24</span><span class="p">(</span><span class="n">input_t</span><span class="p">,</span> <span class="o">*</span><span class="n">input_t</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">w_24</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>


<span class="nd">@register_layout</span><span class="p">(</span><span class="n">PlainLayout</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">PlainAQTTensorImpl</span><span class="p">(</span><span class="n">AQTTensorImpl</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    TensorImpl for plain layout for affine quantized tensor, it stores int_data, scale, zero_point</span>
<span class="sd">    tensors directly as plain tensors.</span>

<span class="sd">    fields:</span>
<span class="sd">      int_data (torch.Tensor): the quantized integer data Tensor</span>
<span class="sd">      scale (torch.Tensor): the scale Tensor used to map between floating point tensor to quantized tensor</span>
<span class="sd">      zero_point (torch.Tensor): the zero_point Tensor used to map between floating point tensor to quantized tensor</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__new__</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">int_data</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">zero_point</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">_layout</span><span class="p">:</span> <span class="n">Layout</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;device&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">int_data</span><span class="o">.</span><span class="n">device</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;layout&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;layout&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;layout&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="k">else</span> <span class="n">int_data</span><span class="o">.</span><span class="n">layout</span>
        <span class="p">)</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;dtype&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">int_data</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;requires_grad&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">int_data</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_make_wrapper_subclass</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">int_data</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">zero_point</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">_layout</span><span class="p">:</span> <span class="n">Layout</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">int_data</span> <span class="o">=</span> <span class="n">int_data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">zero_point</span> <span class="o">=</span> <span class="n">zero_point</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span> <span class="o">=</span> <span class="n">_layout</span>

    <span class="k">def</span> <span class="nf">__tensor_flatten__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="s2">&quot;int_data&quot;</span><span class="p">,</span> <span class="s2">&quot;scale&quot;</span><span class="p">,</span> <span class="s2">&quot;zero_point&quot;</span><span class="p">],</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="p">]</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">__tensor_unflatten__</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span> <span class="n">tensor_data_dict</span><span class="p">,</span> <span class="n">tensor_attributes</span><span class="p">,</span> <span class="n">outer_size</span><span class="p">,</span> <span class="n">outer_stride</span>
    <span class="p">):</span>
        <span class="n">int_data</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span> <span class="o">=</span> <span class="n">tensor_data_dict</span><span class="p">[</span><span class="s2">&quot;int_data&quot;</span><span class="p">],</span> <span class="n">tensor_data_dict</span><span class="p">[</span><span class="s2">&quot;scale&quot;</span><span class="p">],</span> <span class="n">tensor_data_dict</span><span class="p">[</span><span class="s2">&quot;zero_point&quot;</span><span class="p">]</span>
        <span class="n">_layout</span><span class="p">,</span> <span class="o">=</span> <span class="n">tensor_attributes</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">int_data</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span><span class="p">,</span> <span class="n">_layout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_to_kwargs</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">int_data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;device&quot;</span><span class="p">]),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;device&quot;</span><span class="p">]),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">zero_point</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;device&quot;</span><span class="p">]),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_apply_fn_to_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fn</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">(</span>
            <span class="n">fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">int_data</span><span class="p">),</span>
            <span class="n">fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">),</span>
            <span class="n">fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">zero_point</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">__torch_dispatch__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span> <span class="k">if</span> <span class="n">kwargs</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">kwargs</span>

        <span class="k">if</span> <span class="n">func</span> <span class="ow">is</span> <span class="n">aten</span><span class="o">.</span><span class="n">detach</span><span class="o">.</span><span class="n">default</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">return_and_correct_aliasing</span><span class="p">(</span>
                <span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_apply_fn_to_data</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">detach</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">func</span> <span class="ow">is</span> <span class="n">aten</span><span class="o">.</span><span class="n">clone</span><span class="o">.</span><span class="n">default</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">return_and_correct_aliasing</span><span class="p">(</span>
                <span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_apply_fn_to_data</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">clone</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="k">elif</span> <span class="n">func</span> <span class="ow">is</span> <span class="n">aten</span><span class="o">.</span><span class="n">t</span><span class="o">.</span><span class="n">default</span><span class="p">:</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">new</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="vm">__class__</span><span class="p">(</span>
                <span class="n">tensor</span><span class="o">.</span><span class="n">int_data</span><span class="o">.</span><span class="n">t</span><span class="p">(),</span> <span class="n">tensor</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span> <span class="n">tensor</span><span class="o">.</span><span class="n">zero_point</span><span class="p">,</span> <span class="n">tensor</span><span class="o">.</span><span class="n">_layout</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">return_and_correct_aliasing</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">new</span><span class="p">)</span>

        <span class="k">elif</span> <span class="n">func</span> <span class="ow">is</span> <span class="n">aten</span><span class="o">.</span><span class="n">slice</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">step</span> <span class="o">=</span> <span class="n">fill_defaults</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">return_and_correct_aliasing</span><span class="p">(</span>
                    <span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_apply_fn_to_data</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">aten</span><span class="o">.</span><span class="n">slice</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">step</span><span class="p">))</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;slice dim==1 only works when len(scale.shape) == 1 currently, got: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="k">return</span> <span class="n">PlainAQTTensorImpl</span><span class="p">(</span><span class="n">aten</span><span class="o">.</span><span class="n">slice</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">int_data</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">step</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero_point</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;PlainAQTTensorImpl dispatch: attempting to run </span><span class="si">{</span><span class="n">func</span><span class="si">}</span><span class="s2">, with dim=</span><span class="si">{</span><span class="n">dim</span><span class="si">}</span><span class="s2">, that is not supported&quot;</span><span class="p">)</span>

        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;PlainAQTTensorImpl dispatch: attempting to run </span><span class="si">{</span><span class="n">func</span><span class="si">}</span><span class="s2">, this is not supported&quot;</span>
        <span class="p">)</span>

    <span class="n">__torch_function__</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_disabled_torch_function_impl</span>

    <span class="k">def</span> <span class="nf">get_plain</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">int_data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero_point</span>

    <span class="k">def</span> <span class="nf">get_layout</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Layout</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_plain</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">int_data</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">_layout</span><span class="p">:</span> <span class="n">Layout</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">_layout</span><span class="p">,</span> <span class="n">PlainLayout</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">int_data</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span><span class="p">,</span> <span class="n">_layout</span><span class="p">)</span>

<span class="nd">@register_layout</span><span class="p">(</span><span class="n">SemiSparseLayout</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">SemiSparseAQTTensorImpl</span><span class="p">(</span><span class="n">PlainAQTTensorImpl</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    TensorImpl for semi_sparse_cusparselt layout for affine quantized tensor</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">__torch_dispatch__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span> <span class="k">if</span> <span class="n">kwargs</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">kwargs</span>

        <span class="k">if</span> <span class="n">func</span> <span class="ow">is</span> <span class="n">aten</span><span class="o">.</span><span class="n">detach</span><span class="o">.</span><span class="n">default</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">return_and_correct_aliasing</span><span class="p">(</span>
                <span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_apply_fn_to_data</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">detach</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;SparseAQTTensorImpl dispatch: attempting to run </span><span class="si">{</span><span class="n">func</span><span class="si">}</span><span class="s2">, this is not supported&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_plain</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Currently we don&#39;t have cuSPARSELt expansion routines, so we matmul by</span>
        <span class="c1"># the identity matrix to get the original dense matrix. This is slow though.</span>
        <span class="n">cols</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">int_data</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">//</span> <span class="p">(</span><span class="mi">10</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">int_data_expanded</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_cslt_sparse_mm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">int_data</span><span class="p">,</span>
                                                  <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">cols</span><span class="p">,</span>
                                                            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">int_data</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                                                            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">int_data</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">t</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">int_data_expanded</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero_point</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_plain</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">int_data</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">_layout</span><span class="p">:</span> <span class="n">Layout</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">_layout</span><span class="p">,</span> <span class="n">SemiSparseLayout</span><span class="p">)</span>
        <span class="n">int_data_compressed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_cslt_compress</span><span class="p">(</span><span class="n">int_data</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">int_data_compressed</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span><span class="p">,</span> <span class="n">_layout</span><span class="p">)</span>

<span class="nd">@register_layout</span><span class="p">(</span><span class="n">BlockSparseLayout</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">BlockSparseAQTTensorImpl</span><span class="p">(</span><span class="n">PlainAQTTensorImpl</span><span class="p">):</span>
    <span class="n">bsr_crow_indices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
    <span class="n">bsr_col_indices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
    <span class="n">bsr_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
    <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>

    <span class="vm">__slots__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;bsr_crow_indices&quot;</span><span class="p">,</span> <span class="s2">&quot;bsr_col_indices&quot;</span><span class="p">,</span> <span class="s2">&quot;bsr_values&quot;</span><span class="p">,</span> <span class="s2">&quot;scale&quot;</span><span class="p">,</span> <span class="s2">&quot;zero_point&quot;</span><span class="p">]</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="fm">__new__</span><span class="p">(</span>  <span class="c1"># noqa: PYI034</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">shape</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">,</span>
        <span class="n">bsr_crow_indices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">bsr_col_indices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">bsr_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">scale</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">_layout</span><span class="p">:</span> <span class="n">Layout</span><span class="p">,</span>
        <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">bsr_values</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;bsr values must be provided!&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">previous_tensor</span> <span class="o">=</span> <span class="n">bsr_values</span>

        <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="n">previous_tensor</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="s2">&quot;dtype&quot;</span><span class="p">:</span> <span class="n">previous_tensor</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="s2">&quot;layout&quot;</span><span class="p">:</span> <span class="n">previous_tensor</span><span class="o">.</span><span class="n">layout</span><span class="p">,</span>
            <span class="s2">&quot;requires_grad&quot;</span><span class="p">:</span> <span class="n">requires_grad</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_make_wrapper_subclass</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>  <span class="c1"># noqa: PYI034</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">shape</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">,</span>
        <span class="n">bsr_crow_indices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">bsr_col_indices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">bsr_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">scale</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">_layout</span><span class="p">:</span> <span class="n">Layout</span><span class="p">,</span>
        <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bsr_crow_indices</span> <span class="o">=</span> <span class="n">bsr_crow_indices</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bsr_col_indices</span> <span class="o">=</span> <span class="n">bsr_col_indices</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bsr_values</span> <span class="o">=</span> <span class="n">bsr_values</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">zero_point</span> <span class="o">=</span> <span class="n">zero_point</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span> <span class="o">=</span> <span class="n">_layout</span>

    <span class="k">def</span> <span class="nf">__tensor_flatten__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">inner_tensors</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span>
            <span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__slots__</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">tensor_meta</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">inner_tensors</span><span class="p">,</span> <span class="n">tensor_meta</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">__tensor_unflatten__</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">inner_tensors</span><span class="p">,</span>
        <span class="n">tensor_meta</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">,</span> <span class="nb">bool</span><span class="p">],</span>
        <span class="n">outer_size</span><span class="p">,</span>
        <span class="n">outer_stride</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">shape</span><span class="p">,</span> <span class="n">_layout</span><span class="p">,</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="n">tensor_meta</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span>
            <span class="n">bsr_crow_indices</span><span class="o">=</span><span class="n">inner_tensors</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;bsr_crow_indices&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
            <span class="n">bsr_col_indices</span><span class="o">=</span><span class="n">inner_tensors</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;bsr_col_indices&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
            <span class="n">bsr_values</span><span class="o">=</span><span class="n">inner_tensors</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;bsr_values&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
            <span class="n">scale</span><span class="o">=</span><span class="n">inner_tensors</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;scale&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
            <span class="n">zero_point</span><span class="o">=</span><span class="n">inner_tensors</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;zero_point&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
            <span class="n">_layout</span><span class="o">=</span><span class="n">_layout</span><span class="p">,</span>
            <span class="n">requires_grad</span><span class="o">=</span><span class="n">requires_grad</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_plain</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">int_data</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span><span class="p">,</span> <span class="n">_layout</span><span class="p">):</span>
        <span class="n">bsr_tensor</span> <span class="o">=</span> <span class="n">int_data</span><span class="o">.</span><span class="n">to_sparse_bsr</span><span class="p">(</span><span class="n">_layout</span><span class="o">.</span><span class="n">blocksize</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="n">int_data</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
            <span class="n">bsr_crow_indices</span><span class="o">=</span><span class="n">bsr_tensor</span><span class="o">.</span><span class="n">crow_indices</span><span class="p">(),</span>
            <span class="n">bsr_col_indices</span><span class="o">=</span><span class="n">bsr_tensor</span><span class="o">.</span><span class="n">col_indices</span><span class="p">(),</span>
            <span class="n">bsr_values</span><span class="o">=</span><span class="n">bsr_tensor</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span>
            <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span>
            <span class="n">zero_point</span><span class="o">=</span><span class="n">zero_point</span><span class="p">,</span>
            <span class="n">_layout</span> <span class="o">=</span> <span class="n">_layout</span><span class="p">,</span>
            <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_plain</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">int_data_expanded</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">blocksparse</span><span class="o">.</span><span class="n">bsr_to_dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">crow_indices</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">int_data_expanded</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero_point</span>

    <span class="k">def</span> <span class="nf">_apply_fn_to_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">(</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
            <span class="n">bsr_crow_indices</span><span class="o">=</span><span class="n">func</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bsr_crow_indices</span><span class="p">),</span>
            <span class="n">bsr_col_indices</span><span class="o">=</span><span class="n">func</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bsr_col_indices</span><span class="p">),</span>
            <span class="n">bsr_values</span><span class="o">=</span><span class="n">func</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bsr_values</span><span class="p">),</span>
            <span class="n">scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span>
            <span class="n">zero_point</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">zero_point</span><span class="p">,</span>
            <span class="n">_layout</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="p">,</span>
            <span class="n">requires_grad</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">__torch_dispatch__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span> <span class="k">if</span> <span class="n">kwargs</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">kwargs</span>

        <span class="k">if</span> <span class="n">func</span> <span class="ow">is</span> <span class="n">aten</span><span class="o">.</span><span class="n">detach</span><span class="o">.</span><span class="n">default</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">return_and_correct_aliasing</span><span class="p">(</span>
                <span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_apply_fn_to_data</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">detach</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">func</span> <span class="ow">is</span> <span class="n">aten</span><span class="o">.</span><span class="n">clone</span><span class="o">.</span><span class="n">default</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">return_and_correct_aliasing</span><span class="p">(</span>
                <span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_apply_fn_to_data</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">clone</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="c1"># Need the following for bsr specific functions</span>
        <span class="k">if</span> <span class="n">func</span> <span class="ow">is</span> <span class="n">aten</span><span class="o">.</span><span class="n">crow_indices</span><span class="o">.</span><span class="n">default</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">bsr_crow_indices</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">func</span> <span class="ow">is</span> <span class="n">aten</span><span class="o">.</span><span class="n">col_indices</span><span class="o">.</span><span class="n">default</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">bsr_col_indices</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">func</span> <span class="ow">is</span> <span class="n">aten</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">default</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">bsr_values</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">func</span> <span class="ow">is</span> <span class="n">aten</span><span class="o">.</span><span class="n">_nnz</span><span class="o">.</span><span class="n">default</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">bsr_values</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;BlockSparseAQTTensorImpl dispatch: attempting to run </span><span class="si">{</span><span class="n">func</span><span class="si">}</span><span class="s2">, this is not supported&quot;</span>
        <span class="p">)</span>

<span class="nd">@register_layout</span><span class="p">(</span><span class="n">MarlinSparseLayout</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">MarlinSparseAQTTensorImpl</span><span class="p">(</span><span class="n">AQTTensorImpl</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    TensorImpl for sparse_marlin_24 layout for affine quantized tensor.</span>

<span class="sd">    Can be used with 4 bits and 8 bits quantization.</span>

<span class="sd">    Original marlin documentation and information:</span>
<span class="sd">    https://github.com/IST-DASLab/marlin/tree/master</span>

<span class="sd">    Sparse marlin documentation and information:</span>
<span class="sd">    https://github.com/IST-DASLab/Sparse-Marlin?tab=readme-ov-file</span>

<span class="sd">    fields:</span>
<span class="sd">        original_shape (torch.Size): the original shape of the tensor. used to unpack the tensor to the original shape</span>
<span class="sd">        group_size (int): the group size used to pack the tensor</span>
<span class="sd">        num_bits (int): the number of bits used to quantize the tensor</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="fm">__new__</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">int_data</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">zero_point</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">meta</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">_layout</span><span class="p">:</span> <span class="n">Layout</span><span class="p">,</span>
        <span class="n">original_shape</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">,</span>
        <span class="n">group_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_bits</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;device&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">int_data</span><span class="o">.</span><span class="n">device</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;layout&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;layout&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;layout&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="k">else</span> <span class="n">int_data</span><span class="o">.</span><span class="n">layout</span>
        <span class="p">)</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;dtype&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">int_data</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;requires_grad&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">int_data</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_make_wrapper_subclass</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">int_data</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">zero_point</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">meta</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">_layout</span><span class="p">:</span> <span class="n">Layout</span><span class="p">,</span>
        <span class="n">original_shape</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">,</span>
        <span class="n">group_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_bits</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">int_data</span> <span class="o">=</span> <span class="n">int_data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">zero_point</span> <span class="o">=</span> <span class="n">zero_point</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">meta</span> <span class="o">=</span> <span class="n">meta</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span> <span class="o">=</span> <span class="n">_layout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">original_shape</span> <span class="o">=</span> <span class="n">original_shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">group_size</span> <span class="o">=</span> <span class="n">group_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span> <span class="o">=</span> <span class="n">num_bits</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">__torch_dispatch__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span> <span class="k">if</span> <span class="n">kwargs</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">kwargs</span>

        <span class="k">if</span> <span class="n">func</span> <span class="ow">is</span> <span class="n">aten</span><span class="o">.</span><span class="n">detach</span><span class="o">.</span><span class="n">default</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">return_and_correct_aliasing</span><span class="p">(</span>
                <span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_apply_fn_to_data</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">detach</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;MarlinSparseAQTTensorImpl dispatch: attempting to run </span><span class="si">{</span><span class="n">func</span><span class="si">}</span><span class="s2">, this is not supported&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">__tensor_flatten__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="s2">&quot;int_data&quot;</span><span class="p">,</span> <span class="s2">&quot;scale&quot;</span><span class="p">,</span> <span class="s2">&quot;zero_point&quot;</span><span class="p">,</span> <span class="s2">&quot;meta&quot;</span><span class="p">],</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">original_shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">group_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="p">]</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">__tensor_unflatten__</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span> <span class="n">tensor_data_dict</span><span class="p">,</span> <span class="n">tensor_attributes</span><span class="p">,</span> <span class="n">outer_size</span><span class="p">,</span> <span class="n">outer_stride</span>
    <span class="p">):</span>
        <span class="n">int_data</span> <span class="o">=</span> <span class="n">tensor_data_dict</span><span class="p">[</span><span class="s2">&quot;int_data&quot;</span><span class="p">]</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">tensor_data_dict</span><span class="p">[</span><span class="s2">&quot;scale&quot;</span><span class="p">]</span>
        <span class="n">zero_point</span> <span class="o">=</span> <span class="n">tensor_data_dict</span><span class="p">[</span><span class="s2">&quot;zero_point&quot;</span><span class="p">]</span>
        <span class="n">meta</span> <span class="o">=</span> <span class="n">tensor_data_dict</span><span class="p">[</span><span class="s2">&quot;meta&quot;</span><span class="p">]</span>
        <span class="n">_layout</span><span class="p">,</span> <span class="n">original_shape</span><span class="p">,</span> <span class="n">group_size</span><span class="p">,</span> <span class="n">num_bits</span> <span class="o">=</span> <span class="n">tensor_attributes</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">int_data</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span><span class="p">,</span> <span class="n">meta</span><span class="p">,</span> <span class="n">_layout</span><span class="p">,</span> <span class="n">original_shape</span><span class="p">,</span> <span class="n">group_size</span><span class="p">,</span> <span class="n">num_bits</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_plain</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">torchao.sparsity.marlin</span> <span class="kn">import</span> <span class="n">unpack_from_marlin_24</span>  <span class="c1"># avoid circular import</span>
        <span class="n">int_data_expanded</span><span class="p">,</span> <span class="n">scales_expanded</span> <span class="o">=</span> <span class="n">unpack_from_marlin_24</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">int_data</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">meta</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">original_shape</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">group_size</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">int_data_expanded_t</span> <span class="o">=</span> <span class="n">int_data_expanded</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
        <span class="n">scales_expanded_t</span> <span class="o">=</span> <span class="n">scales_expanded</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">int_data_expanded_t</span><span class="p">,</span> <span class="n">scales_expanded_t</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero_point</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_plain</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">int_data</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">zero_point</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">_layout</span><span class="p">:</span> <span class="n">Layout</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="kn">from</span> <span class="nn">torchao.sparsity.marlin</span> <span class="kn">import</span> <span class="n">pack_to_marlin_24</span><span class="p">,</span> <span class="n">const</span>  <span class="c1"># avoid circular import</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">_layout</span><span class="p">,</span> <span class="n">MarlinSparseLayout</span><span class="p">)</span>

        <span class="c1"># Linear layers are (in_features, out_features) but the int_data that is reaching this point</span>
        <span class="c1"># is (out_features, in_features). We need to transpose it to match the expected shape in the marlin code.</span>
        <span class="n">q_w_24</span> <span class="o">=</span> <span class="n">int_data</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
        <span class="n">scale_t</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_capability</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">8</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s1">&#39;Can not use Sparse Marlin 2:4 int4*fp16 kernel with a device of compute capability </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_capability</span><span class="p">()</span><span class="si">}</span><span class="s1">, the minimum compute capability is 8.0 for Marlin kernel.&#39;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">q_w_24</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Only `torch.int32` weights are supported.&quot;</span><span class="p">)</span>

        <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span> <span class="o">=</span> <span class="n">q_w_24</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">if</span> <span class="n">in_features</span> <span class="o">%</span> <span class="mi">128</span> <span class="o">!=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">out_features</span> <span class="o">!=</span> <span class="mi">256</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`in_features` must be divisible by 64 and `out_features` by 256.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># NOTE: The current marlin 2:4 kernel supports both 4 and 8 bits quantization but fp8</span>
        <span class="c1"># will require a bit more work to get our current quantization flow to work with it.</span>
        <span class="c1"># Check the link for a reference: https://github.com/neuralmagic/nm-vllm/tree/main</span>
        <span class="n">num_bits</span> <span class="o">=</span> <span class="mi">4</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">q_w_24</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">16</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span>
        <span class="k">if</span> <span class="n">num_bits</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">4</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Only </span><span class="si">{</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="si">}</span><span class="s2"> bits are supported, got </span><span class="si">{</span><span class="n">num_bits</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

        <span class="n">group_size</span> <span class="o">=</span> <span class="n">in_features</span> <span class="o">//</span> <span class="n">scale_t</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">group_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">group_size</span> <span class="o">=</span> <span class="n">in_features</span>
        <span class="k">assert</span> <span class="n">group_size</span> <span class="o">&lt;=</span> <span class="n">in_features</span><span class="p">,</span> <span class="s2">&quot;Group size must be less than or equal to in_features.&quot;</span>

        <span class="k">if</span> <span class="n">group_size</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">const</span><span class="o">.</span><span class="n">SUPPORTED_GROUP_SIZES</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Only </span><span class="si">{</span><span class="n">const</span><span class="o">.</span><span class="n">SUPPORTED_GROUP_SIZES</span><span class="si">}</span><span class="s2"> group sizes are supported, got </span><span class="si">{</span><span class="n">group_size</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Compress quantized weight to marlin 2:4 format</span>
        <span class="n">marlin_24_q_w_comp</span><span class="p">,</span> <span class="n">marlin_24_s</span><span class="p">,</span> <span class="n">meta</span> <span class="o">=</span> <span class="n">pack_to_marlin_24</span><span class="p">(</span><span class="n">q_w_24</span><span class="p">,</span> <span class="n">scale_t</span><span class="p">,</span> <span class="n">num_bits</span><span class="p">,</span> <span class="n">group_size</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span>
            <span class="n">marlin_24_q_w_comp</span><span class="p">,</span> <span class="n">marlin_24_s</span><span class="p">,</span> <span class="n">zero_point</span><span class="p">,</span>
            <span class="n">meta</span><span class="p">,</span> <span class="n">_layout</span><span class="p">,</span> <span class="n">q_w_24</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
            <span class="n">group_size</span><span class="p">,</span> <span class="n">num_bits</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_layout</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Layout</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span>

    <span class="k">def</span> <span class="nf">_apply_fn_to_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fn</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">int_data</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">int_data</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">zero_point</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">zero_point</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">meta</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">meta</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>


<span class="nd">@register_layout</span><span class="p">(</span><span class="n">Float8Layout</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">Float8AQTTensorImpl</span><span class="p">(</span><span class="n">AQTTensorImpl</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    TensorImpl for float8 layout affine quantized tensor</span>

<span class="sd">    Note: technically we should not create a new layout for float8 we should merge this into</span>
<span class="sd">    plain layout</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">float8_data</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
    <span class="n">transposed</span><span class="p">:</span> <span class="nb">bool</span>

    <span class="k">def</span> <span class="fm">__new__</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">float8_data</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">transposed</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">_layout</span><span class="p">:</span> <span class="n">Layout</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;device&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">float8_data</span><span class="o">.</span><span class="n">device</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;layout&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;layout&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;layout&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="k">else</span> <span class="n">float8_data</span><span class="o">.</span><span class="n">layout</span>
        <span class="p">)</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;dtype&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">float8_data</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;requires_grad&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">float8_data</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_make_wrapper_subclass</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">float8_data</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">transposed</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">_layout</span><span class="p">:</span> <span class="n">Layout</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">float8_data</span> <span class="o">=</span> <span class="n">float8_data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transposed</span> <span class="o">=</span> <span class="n">transposed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span> <span class="o">=</span> <span class="n">_layout</span>

    <span class="k">def</span> <span class="nf">_apply_fn_to_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fn</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Applys a fn to all tensor components stored on this class&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">(</span>
            <span class="n">fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">float8_data</span><span class="p">),</span>
            <span class="n">fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">transposed</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_to_kwargs</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">float8_data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;device&quot;</span><span class="p">]),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;device&quot;</span><span class="p">]),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">transposed</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">__tensor_flatten__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="s2">&quot;float8_data&quot;</span><span class="p">,</span> <span class="s2">&quot;scale&quot;</span><span class="p">],</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">transposed</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="p">]</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">__tensor_unflatten__</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span> <span class="n">tensor_data_dict</span><span class="p">,</span> <span class="n">tensor_attributes</span><span class="p">,</span> <span class="n">outer_size</span><span class="p">,</span> <span class="n">outer_stride</span>
    <span class="p">):</span>
        <span class="n">float8_data</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">tensor_data_dict</span><span class="p">[</span><span class="s2">&quot;float8_data&quot;</span><span class="p">],</span> <span class="n">tensor_data_dict</span><span class="p">[</span><span class="s2">&quot;scale&quot;</span><span class="p">]</span>
        <span class="n">transposed</span><span class="p">,</span> <span class="n">_layout</span><span class="p">,</span> <span class="o">=</span> <span class="n">tensor_attributes</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">float8_data</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">transposed</span><span class="p">,</span> <span class="n">_layout</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">__torch_dispatch__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span> <span class="k">if</span> <span class="n">kwargs</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">kwargs</span>

        <span class="k">if</span> <span class="n">func</span> <span class="ow">is</span> <span class="n">aten</span><span class="o">.</span><span class="n">detach</span><span class="o">.</span><span class="n">default</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">return_and_correct_aliasing</span><span class="p">(</span>
                <span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_apply_fn_to_data</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">detach</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">func</span> <span class="ow">is</span> <span class="n">aten</span><span class="o">.</span><span class="n">clone</span><span class="o">.</span><span class="n">default</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">return_and_correct_aliasing</span><span class="p">(</span>
                <span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_apply_fn_to_data</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">clone</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">func</span> <span class="ow">is</span> <span class="n">aten</span><span class="o">.</span><span class="n">t</span><span class="o">.</span><span class="n">default</span><span class="p">:</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;we don&#39;t need to repack the weight and just rely on external</span>
<span class="sd">            shape being changed and record the status of transpose/no-transpose</span>
<span class="sd">            &quot;&quot;&quot;</span>
            <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">transposed</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">transposed</span>
            <span class="k">return</span> <span class="n">return_and_correct_aliasing</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">elif</span> <span class="n">func</span> <span class="ow">is</span> <span class="n">aten</span><span class="o">.</span><span class="n">slice</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">step</span> <span class="o">=</span> <span class="n">fill_defaults</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1">#TODO: scale replecation should be dependent on block size</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">return_and_correct_aliasing</span><span class="p">(</span>
                        <span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_apply_fn_to_data</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">aten</span><span class="o">.</span><span class="n">slice</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">step</span><span class="p">))</span>
                    <span class="p">)</span>
                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">return_and_correct_aliasing</span><span class="p">(</span>
                        <span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">Float8AQTTensorImpl</span><span class="p">(</span><span class="n">aten</span><span class="o">.</span><span class="n">slice</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">float8_data</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">step</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="p">)</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Float8AQTTensorImpl dispatch: attempting to run </span><span class="si">{</span><span class="n">func</span><span class="si">}</span><span class="s2">, with scale ndim=</span><span class="si">{</span><span class="n">dim</span><span class="si">}</span><span class="s2">, that is not supported&quot;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">return_and_correct_aliasing</span><span class="p">(</span>
                        <span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">Float8AQTTensorImpl</span><span class="p">(</span><span class="n">aten</span><span class="o">.</span><span class="n">slice</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">float8_data</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Float8AQTTensorImpl dispatch: attempting to run </span><span class="si">{</span><span class="n">func</span><span class="si">}</span><span class="s2">, with dim=</span><span class="si">{</span><span class="n">dim</span><span class="si">}</span><span class="s2">, that is not supported&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Float8AQTTensorImpl dispatch: attempting to run </span><span class="si">{</span><span class="n">func</span><span class="si">}</span><span class="s2">, this is not supported&quot;</span>
            <span class="p">)</span>

    <span class="n">__torch_function__</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_disabled_torch_function_impl</span>

    <span class="k">def</span> <span class="nf">get_plain</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">float8_data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">get_layout</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Layout</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_plain</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">data</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">_layout</span><span class="p">:</span> <span class="n">Layout</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Main entrypoint for constructing Float8TensorImpl&quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="n">_is_float8_type</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Float8 TensorImpl must be constructed from float8 dtype but got </span><span class="si">{</span><span class="n">data</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">_layout</span><span class="p">,</span> <span class="n">Float8Layout</span><span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Float8 TensorImpl must be constructed from Float8Layout but got </span><span class="si">{</span><span class="n">_layout</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="n">_layout</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">float8_data</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_plain</span><span class="p">()</span>
        <span class="n">_layout</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_layout</span><span class="p">()</span>
        <span class="k">return</span> <span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">(</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;float8_data=</span><span class="si">{</span><span class="n">float8_data</span><span class="si">}</span><span class="s2">,</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;scale=</span><span class="si">{</span><span class="n">scale</span><span class="si">}</span><span class="s2">,</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;transposed=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">transposed</span><span class="si">}</span><span class="s2">, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;_layout=</span><span class="si">{</span><span class="n">_layout</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>


<span class="nd">@register_layout</span><span class="p">(</span><span class="n">TensorCoreTiledLayout</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">TensorCoreTiledAQTTensorImpl</span><span class="p">(</span><span class="n">AQTTensorImpl</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    TensorImpl for tensor_core_tiled layout for affine quantized tensor, this is for int4 only,</span>
<span class="sd">    used by tinygemm kernels `_weight_int4pack_mm`</span>

<span class="sd">    It stores the original tensor of dimension [n][k] (int32 dtype) as packed weight of 4-d tensor of</span>
<span class="sd">    dimension: [n / 8][k / (inner_k_tiles * 16)][32][inner_k_tiles / 2]</span>
<span class="sd">    (unpacked Tensor shape is n * k)</span>
<span class="sd">    where inner_k_tiles is an internal argument for packing function of tensor core tiled layout</span>
<span class="sd">    that can affect the performance of the matmul kernel (defaults to 8)</span>

<span class="sd">    Note: we also pack scale and zero point together here for tinygemm kernel</span>

<span class="sd">    Note: technically tensor core tiled layout should be the layout for the underlying packed weight</span>
<span class="sd">    (int Tensor) but since the scale and zero_point are also packed into the same tensor here which is not used</span>
<span class="sd">    in plain layout, we just created a layout for AQT right now, this could be improved if we split out</span>
<span class="sd">    int4 aqt into a separate tensor subclass</span>

<span class="sd">    fields:</span>
<span class="sd">      packed_weight (torch.Tensor): the 4-d packed tensor in a tensor_core_tiled layout</span>
<span class="sd">      scale_and_zero (torch.Tensor): the combined scale Tensor used to map between floating point tensor to quantized tensor and zero_point Tensor</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__new__</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">packed_weight</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">scale_and_zero</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">transposed</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">_layout</span><span class="p">:</span> <span class="n">Layout</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;device&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">packed_weight</span><span class="o">.</span><span class="n">device</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;layout&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;layout&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;layout&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="k">else</span> <span class="n">packed_weight</span><span class="o">.</span><span class="n">layout</span>
        <span class="p">)</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;dtype&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">packed_weight</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;requires_grad&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">packed_weight</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_make_wrapper_subclass</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">packed_weight</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">scale_and_zero</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">transposed</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">_layout</span><span class="p">:</span> <span class="n">Layout</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">packed_weight</span> <span class="o">=</span> <span class="n">packed_weight</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale_and_zero</span> <span class="o">=</span> <span class="n">scale_and_zero</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transposed</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span> <span class="o">=</span> <span class="n">_layout</span>

    <span class="k">def</span> <span class="nf">__tensor_flatten__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="s2">&quot;packed_weight&quot;</span><span class="p">,</span> <span class="s2">&quot;scale_and_zero&quot;</span><span class="p">],</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">transposed</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="p">]</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">__tensor_unflatten__</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span> <span class="n">tensor_data_dict</span><span class="p">,</span> <span class="n">tensor_attributes</span><span class="p">,</span> <span class="n">outer_size</span><span class="p">,</span> <span class="n">outer_stride</span>
    <span class="p">):</span>
        <span class="n">packed_weight</span><span class="p">,</span> <span class="n">scale_and_zero</span> <span class="o">=</span> <span class="n">tensor_data_dict</span><span class="p">[</span><span class="s2">&quot;packed_weight&quot;</span><span class="p">],</span> <span class="n">tensor_data_dict</span><span class="p">[</span><span class="s2">&quot;scale_and_zero&quot;</span><span class="p">]</span>
        <span class="n">transposed</span><span class="p">,</span> <span class="n">_layout</span><span class="p">,</span> <span class="o">=</span> <span class="n">tensor_attributes</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">packed_weight</span><span class="p">,</span> <span class="n">scale_and_zero</span><span class="p">,</span> <span class="n">transposed</span><span class="p">,</span> <span class="n">_layout</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_plain</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">int_data</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">zero_point</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">_layout</span><span class="p">:</span> <span class="n">Layout</span>
    <span class="p">):</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">_layout</span><span class="p">,</span> <span class="n">TensorCoreTiledLayout</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">TORCH_VERSION_AT_LEAST_2_5</span><span class="p">:</span>
            <span class="n">int_data</span> <span class="o">=</span> <span class="p">(</span><span class="n">int_data</span><span class="p">[::,</span> <span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="mi">4</span> <span class="o">|</span> <span class="n">int_data</span><span class="p">[::,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
            <span class="k">assert</span> <span class="n">int_data</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span> <span class="s2">&quot;torch.ops.aten._convert_weight_to_int4pack in torch 2.5 expects `uint8` dtype&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">int_data</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="s2">&quot;torch.ops.aten._convert_weight_to_int4pack in torch 2.4 expects `int32` dtype&quot;</span>
        <span class="n">packed_weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">_convert_weight_to_int4pack</span><span class="p">(</span><span class="n">int_data</span><span class="p">,</span> <span class="n">_layout</span><span class="o">.</span><span class="n">inner_k_tiles</span><span class="p">)</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">int_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">zero_point</span> <span class="o">=</span> <span class="n">zero_point</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">int_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="kn">from</span> <span class="nn">torchao.quantization.utils</span> <span class="kn">import</span> <span class="n">pack_tinygemm_scales_and_zeros</span>
        <span class="n">scale_and_zero</span> <span class="o">=</span> <span class="n">pack_tinygemm_scales_and_zeros</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">packed_weight</span><span class="p">,</span> <span class="n">scale_and_zero</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="n">_layout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_to_kwargs</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;device&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;TensorCoreTiledAQTTensorImpl is only available for cuda device, can&#39;t convert to </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">packed_weight</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scale_and_zero</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">transposed</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_apply_fn_to_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fn</span><span class="p">):</span>
        <span class="c1"># self.packed_weight = fn(self.packed_weight)</span>
        <span class="c1"># self.scale_and_zero = fn(self.scale_and_zero)</span>
        <span class="c1"># return self</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">(</span>
            <span class="n">fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">packed_weight</span><span class="p">),</span>
            <span class="n">fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scale_and_zero</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">transposed</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">__torch_dispatch__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span> <span class="k">if</span> <span class="n">kwargs</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">kwargs</span>

        <span class="k">if</span> <span class="n">func</span> <span class="ow">is</span> <span class="n">aten</span><span class="o">.</span><span class="n">detach</span><span class="o">.</span><span class="n">default</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">return_and_correct_aliasing</span><span class="p">(</span>
                <span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_apply_fn_to_data</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">detach</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">func</span> <span class="ow">is</span> <span class="n">aten</span><span class="o">.</span><span class="n">clone</span><span class="o">.</span><span class="n">default</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">return_and_correct_aliasing</span><span class="p">(</span>
                <span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_apply_fn_to_data</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">clone</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">func</span> <span class="ow">is</span> <span class="n">aten</span><span class="o">.</span><span class="n">t</span><span class="o">.</span><span class="n">default</span><span class="p">:</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;we don&#39;t need to repack the weight and just rely on external</span>
<span class="sd">            shape being changed and record the status of transpose/no-transpose</span>
<span class="sd">            &quot;&quot;&quot;</span>
            <span class="n">transposed</span> <span class="o">=</span> <span class="n">TensorCoreTiledAQTTensorImpl</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">packed_weight</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scale_and_zero</span><span class="p">,</span> <span class="ow">not</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">transposed</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_layout</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">return_and_correct_aliasing</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">transposed</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">func</span> <span class="ow">is</span> <span class="n">aten</span><span class="o">.</span><span class="n">slice</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">step</span> <span class="o">=</span> <span class="n">fill_defaults</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">int_data</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_plain</span><span class="p">()</span>
                <span class="n">int_data</span> <span class="o">=</span> <span class="n">aten</span><span class="o">.</span><span class="n">slice</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">int_data</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
                <span class="c1"># this is to handle padding</span>
                <span class="n">int_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="o">.</span><span class="n">post_process</span><span class="p">(</span><span class="n">int_data</span><span class="p">)</span>
                <span class="n">sliced</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">from_plain</span><span class="p">(</span><span class="n">int_data</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">return_and_correct_aliasing</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">sliced</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">int_data</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_plain</span><span class="p">()</span>
                <span class="k">assert</span> <span class="n">step</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;Only step == 1 is supported in slicing right now&quot;</span>
                <span class="n">data_len</span> <span class="o">=</span> <span class="n">int_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span>
                <span class="n">scale_len</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span>
                <span class="n">ratio</span> <span class="o">=</span> <span class="n">data_len</span> <span class="o">/</span> <span class="n">scale_len</span>
                <span class="n">start_scale</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">start</span> <span class="o">/</span> <span class="n">ratio</span><span class="p">)</span>
                <span class="n">end_scale</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">end</span> <span class="o">/</span> <span class="n">ratio</span><span class="p">)</span>

                <span class="n">int_data</span> <span class="o">=</span> <span class="n">aten</span><span class="o">.</span><span class="n">slice</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">int_data</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
                <span class="c1"># this is to handle padding</span>
                <span class="n">int_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="o">.</span><span class="n">post_process</span><span class="p">(</span><span class="n">int_data</span><span class="p">)</span>
                <span class="n">scale</span> <span class="o">=</span> <span class="n">aten</span><span class="o">.</span><span class="n">slice</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">start_scale</span><span class="p">,</span> <span class="n">end_scale</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
                <span class="n">zero_point</span> <span class="o">=</span> <span class="n">aten</span><span class="o">.</span><span class="n">slice</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">zero_point</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">start_scale</span><span class="p">,</span> <span class="n">end_scale</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
                <span class="n">sliced</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">from_plain</span><span class="p">(</span><span class="n">int_data</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">sliced</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;TensorCoreTiledAQTTensorImpl dispatch: attempting to run </span><span class="si">{</span><span class="n">func</span><span class="si">}</span><span class="s2">, with dim=</span><span class="si">{</span><span class="n">dim</span><span class="si">}</span><span class="s2">, that is not supported&quot;</span><span class="p">)</span>

        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;TensorCoreTiledAQTTensorImpl dispatch: attempting to run </span><span class="si">{</span><span class="n">func</span><span class="si">}</span><span class="s2">, this is not supported&quot;</span>
        <span class="p">)</span>

    <span class="n">__torch_function__</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_disabled_torch_function_impl</span>

    <span class="k">def</span> <span class="nf">get_plain</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
        <span class="kn">from</span> <span class="nn">torchao.quantization.quant_primitives</span> <span class="kn">import</span> <span class="p">(</span>
            <span class="n">ZeroPointDomain</span><span class="p">,</span>
            <span class="n">quantize_affine</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="kn">from</span> <span class="nn">torchao.quantization.utils</span> <span class="kn">import</span> <span class="n">unpack_tinygemm_scales_and_zeros</span>
        <span class="n">scale</span><span class="p">,</span> <span class="n">zero</span> <span class="o">=</span> <span class="n">unpack_tinygemm_scales_and_zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scale_and_zero</span><span class="p">)</span>

        <span class="n">cur_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">cur_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span>
        <span class="n">inner_k_tiles</span> <span class="o">=</span> <span class="n">cur_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span>
        <span class="n">original_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">cur_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">8</span><span class="p">,</span> <span class="n">cur_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">inner_k_tiles</span> <span class="o">*</span> <span class="mi">16</span><span class="p">))</span>
        <span class="n">eye_shape</span> <span class="o">=</span> <span class="n">original_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">groupsize</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">original_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="n">scale</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>
        <span class="n">block_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">groupsize</span><span class="p">)</span>
        <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
        <span class="n">original_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>
        <span class="n">target_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int32</span>
        <span class="n">quant_min</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">quant_max</span> <span class="o">=</span> <span class="mi">15</span>
        <span class="n">zero_point_domain</span> <span class="o">=</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">FLOAT</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">block_size</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">block_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span>
        <span class="n">dequantized</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">_weight_int4pack_mm</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">eye_shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">original_dtype</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">packed_weight</span><span class="p">,</span> <span class="n">groupsize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_and_zero</span><span class="p">)</span>
        <span class="n">dequantized</span> <span class="o">=</span> <span class="n">dequantized</span><span class="o">.</span><span class="n">t</span><span class="p">()</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="c1"># TODO: move this to `unpack_tinygemm_scales_and_zeros`?</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">scale</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="n">zero</span> <span class="o">=</span> <span class="n">zero</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">zero</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="n">int_data</span> <span class="o">=</span> <span class="n">quantize_affine</span><span class="p">(</span><span class="n">dequantized</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero</span><span class="p">,</span> <span class="n">target_dtype</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">,</span> <span class="n">zero_point_domain</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">int_data</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero</span>

    <span class="k">def</span> <span class="nf">get_layout</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Layout</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span>


<span class="c1">#####################################################</span>
<span class="c1"># torch functional and aten operator implementation #</span>
<span class="c1">#####################################################</span>

<span class="k">def</span> <span class="nf">_aqt_is_int8</span><span class="p">(</span><span class="n">aqt</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check if an AffineQuantizedTensor is int8 quantized Tensor&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="n">aqt</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span> <span class="ow">and</span>
        <span class="p">(</span><span class="n">aqt</span><span class="o">.</span><span class="n">quant_min</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">aqt</span><span class="o">.</span><span class="n">quant_min</span> <span class="o">==</span> <span class="o">-</span><span class="mi">128</span><span class="p">)</span> <span class="ow">and</span>
        <span class="p">(</span><span class="n">aqt</span><span class="o">.</span><span class="n">quant_max</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">aqt</span><span class="o">.</span><span class="n">quant_max</span> <span class="o">==</span> <span class="mi">127</span><span class="p">)</span>
    <span class="p">)</span>

<span class="k">def</span> <span class="nf">_aqt_is_int8_reduced_range</span><span class="p">(</span><span class="n">aqt</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="n">aqt</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span> <span class="ow">and</span>
        <span class="n">aqt</span><span class="o">.</span><span class="n">quant_min</span> <span class="o">==</span> <span class="o">-</span><span class="mi">127</span> <span class="ow">and</span>
        <span class="p">(</span><span class="n">aqt</span><span class="o">.</span><span class="n">quant_max</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">aqt</span><span class="o">.</span><span class="n">quant_max</span> <span class="o">==</span> <span class="mi">127</span><span class="p">)</span>
    <span class="p">)</span>

<span class="k">def</span> <span class="nf">_aqt_is_tensor_core_tile_uint4</span><span class="p">(</span><span class="n">aqt</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check if an AffineQuantizedTensor is uint4 quantized Tensor&quot;&quot;&quot;</span>
    <span class="c1"># TODO: use torch.uint4</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="n">aqt</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">int32</span> <span class="ow">and</span>
        <span class="n">aqt</span><span class="o">.</span><span class="n">quant_min</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span>
        <span class="n">aqt</span><span class="o">.</span><span class="n">quant_max</span> <span class="o">==</span> <span class="mi">15</span>
    <span class="p">)</span>


<span class="n">implements</span> <span class="o">=</span> <span class="n">AffineQuantizedTensor</span><span class="o">.</span><span class="n">implements</span>

<span class="c1"># following are a list of (dispatch_condition, implementation) functions that takes the following args:</span>
<span class="c1"># input_tensor: dimension is (M1, M2, ..., in_features)</span>
<span class="c1"># weight_tensor: dimension is (out_features, in_features)</span>
<span class="c1"># bias: dimension is (out_features,)</span>
<span class="c1"># so that these can be shared by F.linear, aten.mm, aten.addmm dispatches</span>

<span class="k">def</span> <span class="nf">_linear_int8_act_int8_weight_check</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">weight_tensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">AffineQuantizedTensor</span><span class="p">)</span> <span class="ow">and</span>
        <span class="n">_aqt_is_int8_reduced_range</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span> <span class="ow">and</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight_tensor</span><span class="p">,</span> <span class="n">AffineQuantizedTensor</span><span class="p">)</span> <span class="ow">and</span>
        <span class="n">input_tensor</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">weight_tensor</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">and</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_tensor</span><span class="o">.</span><span class="n">_layout</span><span class="p">,</span> <span class="n">PlainLayout</span><span class="p">)</span> <span class="ow">and</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight_tensor</span><span class="o">.</span><span class="n">_layout</span><span class="p">,</span> <span class="n">PlainLayout</span><span class="p">)</span>
    <span class="p">)</span>

<span class="k">def</span> <span class="nf">_linear_int8_act_int8_weight_impl</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">weight_tensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
    <span class="c1">#</span>
    <span class="c1"># 1. do the matrix form of dot(X_i, W_j)</span>
    <span class="c1">#</span>
    <span class="c1">#</span>
    <span class="c1"># 2. rescale the output</span>
    <span class="c1">#</span>
    <span class="c1"># in cases with large matrices, y_dot_int32 can grow sufficiently</span>
    <span class="c1"># large that y_dot_int32 * a float16 scale is greater than the maximum</span>
    <span class="c1"># value of a float 16, (which results in a value of inf even if multiplying</span>
    <span class="c1"># by the other scale would bring it within the expected range)</span>

    <span class="n">x_vals_int8</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">int_data</span>
    <span class="n">x_scales</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">scale</span>
    <span class="n">w_vals_int8_t</span> <span class="o">=</span> <span class="n">weight_tensor</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">int_data</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
    <span class="n">w_scales</span> <span class="o">=</span> <span class="n">weight_tensor</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">scale</span>
    <span class="n">tmp</span> <span class="o">=</span> <span class="n">x_vals_int8</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x_vals_int8</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">x_scales_dtype</span> <span class="o">=</span> <span class="n">x_scales</span><span class="o">.</span><span class="n">dtype</span>
    <span class="c1"># Cast fp16 scale to float to avoid overflow in int_scaled_matmul</span>
    <span class="n">intermediate_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span> <span class="k">if</span> <span class="n">x_scales_dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">half</span> <span class="k">else</span> <span class="n">x_scales_dtype</span>
    <span class="n">y_dot_scaled</span> <span class="o">=</span> <span class="n">int_scaled_matmul</span><span class="p">(</span><span class="n">tmp</span><span class="p">,</span> <span class="n">w_vals_int8_t</span><span class="p">,</span> <span class="n">x_scales</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">intermediate_dtype</span><span class="p">))</span>
    <span class="n">y_dot_scaled</span> <span class="o">=</span> <span class="n">y_dot_scaled</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">x_scales_dtype</span><span class="p">)</span>

    <span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_dot_scaled</span> <span class="o">*</span> <span class="n">w_scales</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
        <span class="o">*</span><span class="n">x_vals_int8</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">y_dot_scaled</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="p">)</span>

    <span class="c1"># can downcast only at the very end</span>
    <span class="n">output_dtype</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">dtype</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">output_dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">+=</span> <span class="n">bias</span>
    <span class="k">return</span> <span class="n">y</span>


<span class="k">def</span> <span class="nf">_linear_int8_act_int8_weight_semi_structured_sparse_check</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">weight_tensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">AffineQuantizedTensor</span><span class="p">)</span> <span class="ow">and</span>
        <span class="n">_aqt_is_int8_reduced_range</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span> <span class="ow">and</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight_tensor</span><span class="p">,</span> <span class="n">AffineQuantizedTensor</span><span class="p">)</span> <span class="ow">and</span>
        <span class="n">weight_tensor</span><span class="o">.</span><span class="n">is_cuda</span> <span class="ow">and</span>
        <span class="n">input_tensor</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">weight_tensor</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">and</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_tensor</span><span class="o">.</span><span class="n">_layout</span><span class="p">,</span> <span class="n">PlainLayout</span><span class="p">)</span> <span class="ow">and</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight_tensor</span><span class="o">.</span><span class="n">_layout</span><span class="p">,</span> <span class="n">SemiSparseLayout</span><span class="p">)</span>
    <span class="p">)</span>

<span class="k">def</span> <span class="nf">_linear_int8_act_int8_weight_semi_structured_sparse_impl</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">weight_tensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
    <span class="n">x_vals_int8</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">int_data</span>
    <span class="n">x_scales</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">scale</span>
    <span class="n">w_vals_int8</span> <span class="o">=</span> <span class="n">weight_tensor</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">int_data</span>
    <span class="n">w_scales</span> <span class="o">=</span> <span class="n">weight_tensor</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">scale</span>
    <span class="n">tmp</span> <span class="o">=</span> <span class="n">x_vals_int8</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x_vals_int8</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="c1"># we fuse one of the scalar matrix multiplications (w_scales) into the sparse mm</span>
    <span class="n">y_dot_bf16_w_scales_fused</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_cslt_sparse_mm</span><span class="p">(</span>
        <span class="n">w_vals_int8</span><span class="p">,</span> <span class="n">tmp</span><span class="o">.</span><span class="n">t</span><span class="p">(),</span> <span class="n">alpha</span><span class="o">=</span><span class="n">w_scales</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">out_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
    <span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_dot_bf16_w_scales_fused</span> <span class="o">*</span> <span class="n">x_scales</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
        <span class="o">*</span><span class="n">x_vals_int8</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">y_dot_bf16_w_scales_fused</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="n">output_dtype</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">dtype</span>
    <span class="c1"># TODO: waiting for jesse&#39;s test/fix</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">output_dtype</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">+=</span> <span class="n">bias</span>
    <span class="k">return</span> <span class="n">y</span>

<span class="k">def</span> <span class="nf">_linear_int8_act_int8_weight_block_sparse_check</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">weight_tensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">AffineQuantizedTensor</span><span class="p">)</span> <span class="ow">and</span>
        <span class="n">_aqt_is_int8_reduced_range</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span> <span class="ow">and</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight_tensor</span><span class="p">,</span> <span class="n">AffineQuantizedTensor</span><span class="p">)</span> <span class="ow">and</span>
        <span class="n">weight_tensor</span><span class="o">.</span><span class="n">is_cuda</span> <span class="ow">and</span>
        <span class="n">input_tensor</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">weight_tensor</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">and</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_tensor</span><span class="o">.</span><span class="n">_layout</span><span class="p">,</span> <span class="n">PlainLayout</span><span class="p">)</span> <span class="ow">and</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight_tensor</span><span class="o">.</span><span class="n">_layout</span><span class="p">,</span> <span class="n">BlockSparseLayout</span><span class="p">)</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">_linear_int8_act_int8_weight_block_sparse_impl</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">weight_tensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
    <span class="n">x_vals_int8</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">int_data</span>
    <span class="n">x_scales</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">scale</span>
    <span class="n">w_vals</span> <span class="o">=</span> <span class="n">weight_tensor</span><span class="o">.</span><span class="n">tensor_impl</span>
    <span class="n">w_scales</span> <span class="o">=</span> <span class="n">weight_tensor</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">scale</span>
    <span class="n">tmp</span> <span class="o">=</span> <span class="n">x_vals_int8</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x_vals_int8</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">tmp_t</span> <span class="o">=</span> <span class="n">tmp</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>

    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">blocksparse</span><span class="o">.</span><span class="n">int_addmm</span><span class="p">(</span><span class="n">w_vals</span><span class="o">.</span><span class="n">crow_indices</span><span class="p">(),</span>
                                        <span class="n">w_vals</span><span class="o">.</span><span class="n">col_indices</span><span class="p">(),</span>
                                        <span class="n">w_vals</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span>
                                        <span class="n">tmp_t</span><span class="p">,</span>
                                        <span class="n">w_scales</span><span class="p">,</span>
                                        <span class="n">x_scales</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">y_shape</span> <span class="o">=</span> <span class="p">(</span><span class="o">*</span><span class="n">x_vals_int8</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">w_scales</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">y_shape</span><span class="p">)</span>

    <span class="c1"># can downcast only at the very end</span>
    <span class="n">output_dtype</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">dtype</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">output_dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">+=</span> <span class="n">bias</span>
    <span class="k">return</span> <span class="n">y</span>


<span class="k">def</span> <span class="nf">_linear_bf16_act_uint4_weight_check</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">weight_tensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="c1"># input is native bfloat16 tensor</span>
        <span class="ow">not</span> <span class="n">is_traceable_wrapper_subclass</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span> <span class="ow">and</span>
        <span class="n">input_tensor</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span> <span class="ow">and</span>
        <span class="c1"># weight is uint4, group quantized tensor_core_tiled tensor impl affine quantized tensor</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight_tensor</span><span class="p">,</span> <span class="n">AffineQuantizedTensor</span><span class="p">)</span> <span class="ow">and</span>
        <span class="n">_aqt_is_tensor_core_tile_uint4</span><span class="p">(</span><span class="n">weight_tensor</span><span class="p">)</span> <span class="ow">and</span>
        <span class="n">weight_tensor</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span> <span class="ow">and</span>
        <span class="nb">len</span><span class="p">(</span><span class="n">weight_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span>
        <span class="n">weight_tensor</span><span class="o">.</span><span class="n">zero_point_domain</span> <span class="o">==</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">FLOAT</span> <span class="ow">and</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight_tensor</span><span class="o">.</span><span class="n">_layout</span><span class="p">,</span> <span class="n">TensorCoreTiledLayout</span><span class="p">)</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">_linear_bf16_act_uint4_weight_impl</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">weight_tensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">weight_tensor</span><span class="o">.</span><span class="n">block_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Requires groupwise quantization, got block_size: </span><span class="si">{</span><span class="n">weight_tensor</span><span class="o">.</span><span class="n">block_size</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">assert</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">weight_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;need input_tensor shape: </span><span class="si">{</span><span class="n">input_tensor</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> final&quot;</span>
        <span class="sa">f</span><span class="s2">&quot;dim to match weight_tensor shape: </span><span class="si">{</span><span class="n">weight_tensor</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> second dim &quot;</span>
    <span class="p">)</span>

    <span class="c1"># TODO: check groupsize quantization</span>
    <span class="c1"># avoid circular dep, TODO: move this to a common util.py</span>
    <span class="n">act_mat</span> <span class="o">=</span> <span class="n">input_tensor</span>
    <span class="c1"># weight is packed from padded (out_features, in_features) weight tensor</span>
    <span class="c1"># (same dimension requirement as F.linear weight)</span>
    <span class="n">packed_weight</span> <span class="o">=</span> <span class="n">weight_tensor</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">packed_weight</span>
    <span class="n">scale_and_zero</span> <span class="o">=</span> <span class="n">weight_tensor</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">scale_and_zero</span>

    <span class="n">orig_act_size</span> <span class="o">=</span> <span class="n">act_mat</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="n">orig_dtype</span> <span class="o">=</span> <span class="n">act_mat</span><span class="o">.</span><span class="n">dtype</span>

    <span class="c1"># reshape and pad activation</span>
    <span class="n">act_mat</span> <span class="o">=</span> <span class="n">act_mat</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">act_mat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>
    <span class="n">pad_size</span> <span class="o">=</span> <span class="n">find_multiple</span><span class="p">(</span><span class="n">act_mat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1024</span><span class="p">)</span>
    <span class="n">act_mat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">act_mat</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">pad_size</span> <span class="o">-</span> <span class="n">act_mat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>

    <span class="c1"># groupwise int4 quantization</span>
    <span class="n">groupsize</span> <span class="o">=</span> <span class="n">weight_tensor</span><span class="o">.</span><span class="n">block_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">_weight_int4pack_mm</span><span class="p">(</span><span class="n">act_mat</span><span class="o">.</span><span class="n">contiguous</span><span class="p">(),</span> <span class="n">packed_weight</span><span class="p">,</span> <span class="n">groupsize</span><span class="p">,</span> <span class="n">scale_and_zero</span><span class="p">)</span>

    <span class="c1"># remove out_feature padding</span>
    <span class="n">orig_out_features</span> <span class="o">=</span> <span class="n">weight_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:,</span> <span class="p">:</span><span class="n">orig_out_features</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">orig_act_size</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">orig_out_features</span><span class="p">)</span>


    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">+=</span> <span class="n">bias</span>
    <span class="k">return</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">orig_dtype</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_linear_fp_act_int8_weight_check</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">weight_tensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="c1"># input is native float tensor</span>
        <span class="ow">not</span> <span class="n">is_traceable_wrapper_subclass</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span> <span class="ow">and</span>
        <span class="n">input_tensor</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="ow">and</span>
        <span class="c1"># weight is int8 per channel quantized affine quantized tensor</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight_tensor</span><span class="p">,</span> <span class="n">AffineQuantizedTensor</span><span class="p">)</span> <span class="ow">and</span>
        <span class="n">_aqt_is_int8</span><span class="p">(</span><span class="n">weight_tensor</span><span class="p">)</span> <span class="ow">and</span>
        <span class="nb">len</span><span class="p">(</span><span class="n">weight_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span>
        <span class="nb">len</span><span class="p">(</span><span class="n">weight_tensor</span><span class="o">.</span><span class="n">block_size</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span>
        <span class="n">weight_tensor</span><span class="o">.</span><span class="n">block_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span>
        <span class="n">weight_tensor</span><span class="o">.</span><span class="n">block_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">weight_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">and</span>
        <span class="n">weight_tensor</span><span class="o">.</span><span class="n">zero_point_domain</span> <span class="o">==</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">INT</span> <span class="ow">and</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight_tensor</span><span class="o">.</span><span class="n">_layout</span><span class="p">,</span> <span class="n">PlainLayout</span><span class="p">)</span>
    <span class="p">)</span>

<span class="k">def</span> <span class="nf">_linear_fp_act_int8_weight_impl</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">weight_tensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
    <span class="c1"># TODO: enable cpu and mps efficient path</span>
    <span class="c1"># is_cpu and is_mps only, some issue with is_contiguous() currently</span>
    <span class="c1"># return torch.ops.aten._weight_int8pack_mm(input_tensor.contiguous(), w_vals_int8_t, weight_tensor.tensor_impl.scale)</span>

    <span class="c1"># per channel int8 weight only quantizated mm</span>
    <span class="n">w_vals_int8_t</span> <span class="o">=</span> <span class="n">weight_tensor</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">int_data</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">weight_tensor</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">scale</span>
    <span class="n">orig_dtype</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">dtype</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span>
        <span class="n">input_tensor</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span>
        <span class="n">w_vals_int8_t</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">input_tensor</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">m</span> <span class="o">*</span> <span class="n">scale</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">input_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">+=</span> <span class="n">bias</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span>

<span class="k">def</span> <span class="nf">_linear_f16_act_floatx_weight_check</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">weight_tensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
    <span class="kn">from</span> <span class="nn">torchao.dtypes.floatx</span> <span class="kn">import</span> <span class="n">FloatxTensorCoreLayout</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="c1"># input is native float32 tensor</span>
        <span class="ow">not</span> <span class="n">is_traceable_wrapper_subclass</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span> <span class="ow">and</span>
        <span class="n">input_tensor</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="ow">and</span>
        <span class="n">input_tensor</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span> <span class="ow">and</span>
        <span class="c1"># weight is floatx Tensor</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight_tensor</span><span class="p">,</span> <span class="n">AffineQuantizedTensor</span><span class="p">)</span> <span class="ow">and</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight_tensor</span><span class="o">.</span><span class="n">_layout</span><span class="p">,</span> <span class="n">FloatxTensorCoreLayout</span><span class="p">)</span> <span class="ow">and</span>
        <span class="p">(</span>
            <span class="c1"># weight is using fp6 quantization</span>
            <span class="p">(</span><span class="n">weight_tensor</span><span class="o">.</span><span class="n">_layout</span><span class="o">.</span><span class="n">ebits</span> <span class="o">==</span> <span class="mi">3</span> <span class="ow">and</span>
             <span class="n">weight_tensor</span><span class="o">.</span><span class="n">_layout</span><span class="o">.</span><span class="n">mbits</span> <span class="o">==</span> <span class="mi">2</span><span class="p">)</span> <span class="ow">or</span>
            <span class="p">(</span><span class="n">weight_tensor</span><span class="o">.</span><span class="n">_layout</span><span class="o">.</span><span class="n">ebits</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span>
             <span class="n">weight_tensor</span><span class="o">.</span><span class="n">_layout</span><span class="o">.</span><span class="n">mbits</span> <span class="o">==</span> <span class="mi">3</span><span class="p">)</span> <span class="ow">or</span>
            <span class="c1"># weight is using fp5 quantization</span>
            <span class="p">(</span><span class="n">weight_tensor</span><span class="o">.</span><span class="n">_layout</span><span class="o">.</span><span class="n">ebits</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span>
             <span class="n">weight_tensor</span><span class="o">.</span><span class="n">_layout</span><span class="o">.</span><span class="n">mbits</span> <span class="o">==</span> <span class="mi">2</span><span class="p">)</span> <span class="ow">or</span>
            <span class="p">(</span><span class="n">weight_tensor</span><span class="o">.</span><span class="n">_layout</span><span class="o">.</span><span class="n">ebits</span> <span class="o">==</span> <span class="mi">3</span> <span class="ow">and</span>
             <span class="n">weight_tensor</span><span class="o">.</span><span class="n">_layout</span><span class="o">.</span><span class="n">mbits</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="p">)</span>

<span class="k">def</span> <span class="nf">_linear_f16_act_floatx_weight_impl</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">weight_tensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
    <span class="kn">from</span> <span class="nn">torchao.dtypes.floatx</span> <span class="kn">import</span> <span class="n">_SPLIT_K_MAP</span>
    <span class="kn">from</span> <span class="nn">torchao.ops</span> <span class="kn">import</span> <span class="n">quant_llm_linear</span>

    <span class="n">act</span> <span class="o">=</span> <span class="n">input_tensor</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">weight_tensor</span>

    <span class="n">out_dim</span><span class="p">,</span> <span class="n">in_dim</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">act_reshaped</span> <span class="o">=</span> <span class="n">act</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">)</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>

    <span class="c1"># https://github.com/microsoft/DeepSpeed/blob/3a3a6db3332e339cc9fd94efd4982f6d60635a3d/deepspeed/inference/v2/kernels/core_ops/cuda_linear/cuda_linear.py</span>
    <span class="n">bsize</span> <span class="o">=</span> <span class="n">act_reshaped</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">splitK</span> <span class="o">=</span> <span class="n">_SPLIT_K_MAP</span><span class="p">[(</span><span class="n">bsize</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">64</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">out_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="n">bsize</span> <span class="o">&lt;=</span> <span class="mi">768</span> <span class="k">else</span> <span class="mi">1</span>

    <span class="n">out</span> <span class="o">=</span> <span class="n">quant_llm_linear</span><span class="p">(</span>
        <span class="n">weight</span><span class="o">.</span><span class="n">_layout</span><span class="o">.</span><span class="n">ebits</span><span class="p">,</span>
        <span class="n">weight</span><span class="o">.</span><span class="n">_layout</span><span class="o">.</span><span class="n">mbits</span><span class="p">,</span>
        <span class="n">act_reshaped</span><span class="p">,</span>
        <span class="n">weight</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">packed_floatx_data</span><span class="p">,</span>
        <span class="n">weight</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span>
        <span class="n">splitK</span><span class="o">=</span><span class="n">splitK</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">+=</span> <span class="n">bias</span>

    <span class="k">return</span> <span class="n">out</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="n">act</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">out_dim</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">act</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_linear_fp8_act_fp8_weight_check</span><span class="p">(</span>
    <span class="n">input_tensor</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">AffineQuantizedTensor</span><span class="p">],</span>
    <span class="n">weight_tensor</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">AffineQuantizedTensor</span><span class="p">],</span>
    <span class="n">bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">check_aqt</span><span class="p">(</span><span class="n">aqt</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">AffineQuantizedTensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">aqt</span><span class="p">,</span> <span class="n">AffineQuantizedTensor</span><span class="p">)</span> <span class="ow">and</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">aqt</span><span class="o">.</span><span class="n">_layout</span><span class="p">,</span> <span class="n">Float8Layout</span><span class="p">)</span>
            <span class="ow">and</span> <span class="n">aqt</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fn</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">float8_e5m2</span><span class="p">]</span>
            <span class="ow">and</span> <span class="p">(</span><span class="n">aqt</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">aqt</span><span class="o">.</span><span class="n">block_size</span> <span class="ow">or</span> <span class="n">_is_rowwise_scaled</span><span class="p">(</span><span class="n">aqt</span><span class="p">))</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="n">check_aqt</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">check_aqt</span><span class="p">(</span><span class="n">weight_tensor</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">preprocess_scale</span><span class="p">(</span><span class="n">input_scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Ensures input tensor is correctly formated for _scaled_mm &quot;&quot;&quot;</span>
    <span class="n">input_scale</span> <span class="o">=</span> <span class="n">input_scale</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">input_scale</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">input_scale</span> <span class="o">=</span> <span class="n">input_scale</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_scale</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">input_scale</span>

<span class="k">def</span> <span class="nf">_linear_fp8_act_fp8_weight_impl</span><span class="p">(</span>
    <span class="n">input_tensor</span><span class="p">:</span> <span class="n">AffineQuantizedTensor</span><span class="p">,</span>
    <span class="n">weight_tensor</span><span class="p">:</span> <span class="n">AffineQuantizedTensor</span><span class="p">,</span>
    <span class="n">bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Implements matmul between FP8 input and FP8 weight with compute using _scaled_mm&quot;&quot;&quot;</span>
    <span class="n">scaled_mm_config</span> <span class="o">=</span> <span class="n">weight_tensor</span><span class="o">.</span><span class="n">_layout</span><span class="o">.</span><span class="n">mm_config</span>
    <span class="n">out_shape</span> <span class="o">=</span> <span class="n">get_out_shape</span><span class="p">(</span><span class="n">input_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">weight_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="c1"># Weight tensor preprocessing</span>
    <span class="n">w_tensor_impl</span> <span class="o">=</span> <span class="n">weight_tensor</span><span class="o">.</span><span class="n">tensor_impl</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="n">w_tensor_impl</span><span class="o">.</span><span class="n">transposed</span><span class="p">,</span> <span class="s2">&quot;Weight tensor must be contiguous&quot;</span>
    <span class="n">w_data</span> <span class="o">=</span> <span class="n">w_tensor_impl</span><span class="o">.</span><span class="n">float8_data</span>
    <span class="n">w_scale</span> <span class="o">=</span> <span class="n">w_tensor_impl</span><span class="o">.</span><span class="n">scale</span>

    <span class="c1"># Input tensor preprocessing</span>
    <span class="n">inpt_data</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">float8_data</span>
    <span class="n">input_scale</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">scale</span>
    <span class="c1"># Handle case where input tensor is more than 2D</span>
    <span class="n">inpt_data</span> <span class="o">=</span> <span class="n">inpt_data</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">inpt_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="c1"># Handle rowwise case</span>
    <span class="k">if</span> <span class="n">_is_rowwise_scaled</span><span class="p">(</span><span class="n">weight_tensor</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">_is_rowwise_scaled</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">),</span> <span class="s2">&quot;Input tensor must be rowwise block size&quot;</span>
        <span class="n">w_scale</span> <span class="o">=</span> <span class="n">w_scale</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
        <span class="n">input_scale</span> <span class="o">=</span> <span class="n">preprocess_scale</span><span class="p">(</span><span class="n">input_scale</span><span class="p">,</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="c1"># Preprocess data</span>
    <span class="n">inpt_data</span><span class="p">,</span> <span class="n">w_data</span> <span class="o">=</span> <span class="n">preprocess_data</span><span class="p">(</span><span class="n">inpt_data</span><span class="p">,</span> <span class="n">w_data</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">scaled_mm_config</span><span class="p">)</span>

    <span class="c1"># Perform the computation</span>
    <span class="k">return</span> <span class="n">addmm_float8_unwrapped_inference</span><span class="p">(</span>
        <span class="n">inpt_data</span><span class="p">,</span>
        <span class="n">input_scale</span><span class="p">,</span>
        <span class="n">w_data</span><span class="p">,</span>
        <span class="n">w_scale</span><span class="p">,</span>
        <span class="n">output_dtype</span><span class="o">=</span><span class="n">input_tensor</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
        <span class="n">use_fast_accum</span><span class="o">=</span><span class="n">scaled_mm_config</span><span class="o">.</span><span class="n">use_fast_accum</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">out_shape</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_linear_fp_act_fp8_weight_check</span><span class="p">(</span>
    <span class="n">input_tensor</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">AffineQuantizedTensor</span><span class="p">],</span>
    <span class="n">weight_tensor</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">AffineQuantizedTensor</span><span class="p">],</span>
    <span class="n">bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="c1"># input is native float tensor</span>
        <span class="ow">not</span> <span class="n">is_traceable_wrapper_subclass</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span> <span class="ow">and</span>
        <span class="n">input_tensor</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="ow">and</span>
        <span class="c1"># weight is float8 quantized affine quantized tensor</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight_tensor</span><span class="p">,</span> <span class="n">AffineQuantizedTensor</span><span class="p">)</span> <span class="ow">and</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight_tensor</span><span class="o">.</span><span class="n">_layout</span><span class="p">,</span> <span class="n">Float8Layout</span><span class="p">)</span>
        <span class="ow">and</span> <span class="n">weight_tensor</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fn</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">float8_e5m2</span><span class="p">]</span>
        <span class="ow">and</span> <span class="p">(</span><span class="n">weight_tensor</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">weight_tensor</span><span class="o">.</span><span class="n">block_size</span> <span class="ow">or</span> <span class="n">_is_rowwise_scaled</span><span class="p">(</span><span class="n">weight_tensor</span><span class="p">))</span>
    <span class="p">)</span>

<span class="k">def</span> <span class="nf">_linear_fp_act_fp8_weight_impl</span><span class="p">(</span>
    <span class="n">input_tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">weight_tensor</span><span class="p">:</span> <span class="n">AffineQuantizedTensor</span><span class="p">,</span>
    <span class="n">bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
<span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">weight_tensor</span><span class="o">.</span><span class="n">dequantize</span><span class="p">(),</span> <span class="n">bias</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_linear_fp_act_int4_weight_sparse_marlin_check</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">weight_tensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight_tensor</span><span class="p">,</span> <span class="n">AffineQuantizedTensor</span><span class="p">)</span> <span class="ow">and</span>
        <span class="n">_aqt_is_tensor_core_tile_uint4</span><span class="p">(</span><span class="n">weight_tensor</span><span class="p">)</span> <span class="ow">and</span>
        <span class="n">input_tensor</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span> <span class="ow">and</span>
        <span class="nb">len</span><span class="p">(</span><span class="n">weight_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span>
        <span class="n">weight_tensor</span><span class="o">.</span><span class="n">zero_point_domain</span> <span class="o">==</span> <span class="n">ZeroPointDomain</span><span class="o">.</span><span class="n">INT</span> <span class="ow">and</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight_tensor</span><span class="o">.</span><span class="n">_layout</span><span class="p">,</span> <span class="n">MarlinSparseLayout</span><span class="p">)</span>
    <span class="p">)</span>

<span class="k">def</span> <span class="nf">_linear_fp_act_int4_weight_sparse_marlin_impl</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">weight_tensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
    <span class="kn">from</span> <span class="nn">torchao.sparsity.marlin</span> <span class="kn">import</span> <span class="n">marlin_24_workspace</span><span class="p">,</span> <span class="n">const</span>
    <span class="kn">from</span> <span class="nn">torchao.ops</span> <span class="kn">import</span> <span class="n">marlin_24_gemm</span>

    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight_tensor</span><span class="p">,</span> <span class="n">AffineQuantizedTensor</span><span class="p">)</span>

    <span class="n">sparse_w_int4</span> <span class="o">=</span> <span class="n">weight_tensor</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">int_data</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">weight_tensor</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">scale</span>
    <span class="n">meta</span> <span class="o">=</span> <span class="n">weight_tensor</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">meta</span>
    <span class="n">original_shape</span> <span class="o">=</span> <span class="n">weight_tensor</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">original_shape</span>
    <span class="n">num_bits</span> <span class="o">=</span> <span class="n">weight_tensor</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">num_bits</span>

    <span class="c1"># Folds batch dimension into the first dimension</span>
    <span class="n">input_2d</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="n">size_m</span> <span class="o">=</span> <span class="n">input_2d</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">size_n</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">size_k</span> <span class="o">=</span> <span class="n">input_2d</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">workspace_24</span> <span class="o">=</span> <span class="n">marlin_24_workspace</span><span class="p">(</span><span class="n">original_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

    <span class="n">out</span> <span class="o">=</span> <span class="n">marlin_24_gemm</span><span class="p">(</span>
        <span class="n">input_2d</span><span class="p">,</span> <span class="n">sparse_w_int4</span><span class="p">,</span> <span class="n">meta</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span>
        <span class="n">workspace_24</span><span class="p">,</span> <span class="n">num_bits</span><span class="p">,</span> <span class="n">size_m</span><span class="p">,</span> <span class="n">size_n</span><span class="p">,</span> <span class="n">size_k</span>
    <span class="p">)</span>

    <span class="c1"># Unfold the batch dimension</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">input_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">scale</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],))</span>

    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">+=</span> <span class="n">bias</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>


<span class="k">def</span> <span class="nf">_register_aqt_quantized_linear_dispatches</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">dispatch_condition</span><span class="p">,</span> <span class="n">impl</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="p">(</span><span class="n">_linear_int8_act_int8_weight_check</span><span class="p">,</span> <span class="n">_linear_int8_act_int8_weight_impl</span><span class="p">),</span>
        <span class="p">(</span><span class="n">_linear_int8_act_int8_weight_semi_structured_sparse_check</span><span class="p">,</span> <span class="n">_linear_int8_act_int8_weight_semi_structured_sparse_impl</span><span class="p">),</span>
        <span class="p">(</span><span class="n">_linear_int8_act_int8_weight_block_sparse_check</span><span class="p">,</span> <span class="n">_linear_int8_act_int8_weight_block_sparse_impl</span><span class="p">),</span>
        <span class="p">(</span><span class="n">_linear_fp8_act_fp8_weight_check</span><span class="p">,</span> <span class="n">_linear_fp8_act_fp8_weight_impl</span><span class="p">),</span>
        <span class="p">(</span><span class="n">_linear_fp_act_fp8_weight_check</span><span class="p">,</span> <span class="n">_linear_fp_act_fp8_weight_impl</span><span class="p">),</span>
        <span class="p">(</span><span class="n">_linear_bf16_act_uint4_weight_check</span><span class="p">,</span> <span class="n">_linear_bf16_act_uint4_weight_impl</span><span class="p">),</span>
        <span class="p">(</span><span class="n">_linear_fp_act_int8_weight_check</span><span class="p">,</span> <span class="n">_linear_fp_act_int8_weight_impl</span><span class="p">),</span>
        <span class="p">(</span><span class="n">_linear_f16_act_floatx_weight_check</span><span class="p">,</span> <span class="n">_linear_f16_act_floatx_weight_impl</span><span class="p">),</span>
        <span class="p">(</span><span class="n">_linear_fp_act_int4_weight_sparse_marlin_check</span><span class="p">,</span> <span class="n">_linear_fp_act_int4_weight_sparse_marlin_impl</span><span class="p">),</span>
    <span class="p">]:</span>
        <span class="n">register_aqt_quantized_linear_dispatch</span><span class="p">(</span><span class="n">dispatch_condition</span><span class="p">,</span> <span class="n">impl</span><span class="p">)</span>

<span class="n">_register_aqt_quantized_linear_dispatches</span><span class="p">()</span>

<span class="nd">@implements</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">linear</span><span class="p">,</span> <span class="n">aten</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">default</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">_</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
    <span class="n">input_tensor</span><span class="p">,</span> <span class="n">weight_tensor</span><span class="p">,</span> <span class="n">bias</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">args</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">func</span><span class="si">}</span><span class="s2"> is not implemented for non floating point input&quot;</span><span class="p">)</span>

    <span class="c1"># using try/except here so that we can have a general fallback when input_tensor/weight_tensor</span>
    <span class="c1"># is not picked up by any of the dispatch paths in `_quantized_linear_op`, this allows us to</span>
    <span class="c1"># make the branches easier to understand in `_quantized_linear_op`</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">weight_tensor</span><span class="o">.</span><span class="n">_quantized_linear_op</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">weight_tensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
    <span class="k">except</span> <span class="n">QuantizedLinearNotImplementedError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="c1"># fallback path is only called when user did not specify a specfic quantized linear implementation with `_layout.quantized_linear_impl`</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight_tensor</span><span class="p">,</span> <span class="n">AffineQuantizedTensor</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">weight_tensor</span><span class="o">.</span><span class="n">_layout</span><span class="p">,</span> <span class="s2">&quot;quantized_linear_impl&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">weight_tensor</span><span class="o">.</span><span class="n">_layout</span><span class="o">.</span><span class="n">quantized_linear_impl</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">e</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">AffineQuantizedTensor</span><span class="p">):</span>
            <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">dequantize</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight_tensor</span><span class="p">,</span> <span class="n">AffineQuantizedTensor</span><span class="p">):</span>
            <span class="n">weight_tensor</span> <span class="o">=</span> <span class="n">weight_tensor</span><span class="o">.</span><span class="n">dequantize</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">weight_tensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>

<span class="nd">@implements</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">embedding</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
    <span class="c1"># new_arg1 = args[1].dequantize()</span>
    <span class="c1"># return torch.nn.embedding(args[0], new_arg1, *args[2:], **kwargs)</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">tensor_impl</span><span class="p">,</span> <span class="n">PlainAQTTensorImpl</span><span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;embedding only works with PlainAQTTensorImpl but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">tensor_impl</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span> 
    <span class="k">assert</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;padding_idx&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;max_norm&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;scale_grad_by_freq&quot;</span><span class="p">]</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;sparse&quot;</span><span class="p">]</span> <span class="ow">and</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;norm_type&quot;</span><span class="p">]</span><span class="o">==</span><span class="mf">2.0</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">int_data</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">get_plain</span><span class="p">()</span>
    
    <span class="n">sliced_data</span><span class="p">,</span> <span class="n">sliced_scale</span><span class="p">,</span> <span class="n">sliced_zero_point</span> <span class="o">=</span> <span class="n">int_data</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">scale</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">zero_point</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
    <span class="c1"># Block size is expecting 2 dimensions [1, group size] but</span>
    <span class="c1"># batchsize or other dims gets added to sliced_data, sliced_scale and sliced_zero_point so </span>
    <span class="c1"># we need to increase block size to correct dim</span>
    <span class="n">new_blocks</span> <span class="o">=</span> <span class="n">idx</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="o">-</span><span class="mi">1</span>
    <span class="k">return</span> <span class="n">dequantize_affine</span><span class="p">(</span>
        <span class="n">sliced_data</span><span class="p">,</span>
        <span class="n">new_blocks</span><span class="o">*</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="nb">list</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">block_size</span><span class="p">),</span>
        <span class="n">sliced_scale</span><span class="p">,</span>
        <span class="n">sliced_zero_point</span><span class="p">,</span>
        <span class="n">sliced_data</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">quant_min</span><span class="p">,</span>
        <span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">quant_max</span><span class="p">,</span>
        <span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">zero_point_domain</span><span class="p">,</span>
        <span class="n">output_dtype</span><span class="o">=</span><span class="n">sliced_scale</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="p">)</span>

<span class="nd">@implements</span><span class="p">(</span><span class="n">aten</span><span class="o">.</span><span class="n">addmm</span><span class="o">.</span><span class="n">default</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
    <span class="n">input_tensor</span><span class="p">,</span> <span class="n">weight_tensor</span><span class="p">,</span> <span class="n">bias</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">args</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
        <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">func</span><span class="si">}</span><span class="s2"> is not implemented for non floating point input&quot;</span><span class="p">)</span>

    <span class="c1"># using try/except here so that we can have a general fallback when input_tensor/weight_tensor</span>
    <span class="c1"># is not picked up by any of the dispatch paths in `_quantized_linear_op`, this allows us to</span>
    <span class="c1"># make the branches easier to understand in `_quantized_linear_op`</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">weight_tensor</span> <span class="o">=</span> <span class="n">weight_tensor</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">weight_tensor</span><span class="o">.</span><span class="n">_quantized_linear_op</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">weight_tensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
    <span class="k">except</span> <span class="n">QuantizedLinearNotImplementedError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="c1"># fallback path is only called when user did not specify a specfic quantized linear implementation with `_layout.quantized_linear_impl`</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight_tensor</span><span class="p">,</span> <span class="n">AffineQuantizedTensor</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">weight_tensor</span><span class="o">.</span><span class="n">_layout</span><span class="p">,</span> <span class="s2">&quot;quantized_linear_impl&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">weight_tensor</span><span class="o">.</span><span class="n">_layout</span><span class="o">.</span><span class="n">quantized_linear_impl</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">e</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">AffineQuantizedTensor</span><span class="p">):</span>
            <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">dequantize</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight_tensor</span><span class="p">,</span> <span class="n">AffineQuantizedTensor</span><span class="p">):</span>
            <span class="n">weight_tensor</span> <span class="o">=</span> <span class="n">weight_tensor</span><span class="o">.</span><span class="n">dequantize</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">,</span> <span class="n">weight_tensor</span><span class="p">)</span>

<span class="nd">@implements</span><span class="p">(</span><span class="n">aten</span><span class="o">.</span><span class="n">mm</span><span class="o">.</span><span class="n">default</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
    <span class="n">input_tensor</span><span class="p">,</span> <span class="n">weight_tensor</span><span class="p">,</span> <span class="n">bias</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="kc">None</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">func</span><span class="si">}</span><span class="s2"> is not implemented for non floating point input&quot;</span><span class="p">)</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="n">weight_tensor</span> <span class="o">=</span> <span class="n">weight_tensor</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">weight_tensor</span><span class="o">.</span><span class="n">_quantized_linear_op</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">weight_tensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
    <span class="k">except</span> <span class="n">QuantizedLinearNotImplementedError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="c1"># fallback path is only called when user did not specify a specfic quantized linear implementation with `_layout.quantized_linear_impl`</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight_tensor</span><span class="p">,</span> <span class="n">AffineQuantizedTensor</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">weight_tensor</span><span class="o">.</span><span class="n">_layout</span><span class="p">,</span> <span class="s2">&quot;quantized_linear_impl&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">weight_tensor</span><span class="o">.</span><span class="n">_layout</span><span class="o">.</span><span class="n">quantized_linear_impl</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">e</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">AffineQuantizedTensor</span><span class="p">):</span>
            <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">dequantize</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight_tensor</span><span class="p">,</span> <span class="n">AffineQuantizedTensor</span><span class="p">):</span>
            <span class="n">weight_tensor</span> <span class="o">=</span> <span class="n">weight_tensor</span><span class="o">.</span><span class="n">dequantize</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">weight_tensor</span><span class="p">)</span>

<span class="nd">@implements</span><span class="p">(</span><span class="n">aten</span><span class="o">.</span><span class="n">detach</span><span class="o">.</span><span class="n">default</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">return_and_correct_aliasing</span><span class="p">(</span>
        <span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_apply_fn_to_data</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">detach</span><span class="p">)</span>
    <span class="p">)</span>


<span class="nd">@implements</span><span class="p">(</span><span class="n">aten</span><span class="o">.</span><span class="n">clone</span><span class="o">.</span><span class="n">default</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">return_and_correct_aliasing</span><span class="p">(</span>
        <span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_apply_fn_to_data</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">clone</span><span class="p">)</span>
    <span class="p">)</span>


<span class="nd">@implements</span><span class="p">(</span><span class="n">aten</span><span class="o">.</span><span class="n">_to_copy</span><span class="o">.</span><span class="n">default</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">return_and_correct_aliasing</span><span class="p">(</span>
        <span class="n">func</span><span class="p">,</span>
        <span class="n">args</span><span class="p">,</span>
        <span class="n">kwargs</span><span class="p">,</span>
        <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="o">.</span><span class="n">_apply_fn_to_data</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">clone</span><span class="p">),</span>
    <span class="p">)</span>

<span class="nd">@implements</span><span class="p">(</span><span class="n">aten</span><span class="o">.</span><span class="n">t</span><span class="o">.</span><span class="n">default</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
    <span class="n">block_size</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">block_size</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">block_size</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
    <span class="n">transposed_block_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">block_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">block_size</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">new</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="vm">__class__</span><span class="p">(</span>
        <span class="n">tensor</span><span class="o">.</span><span class="n">tensor_impl</span><span class="o">.</span><span class="n">t</span><span class="p">(),</span> <span class="n">transposed_block_size</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">tensor</span><span class="o">.</span><span class="n">quant_min</span><span class="p">,</span> <span class="n">tensor</span><span class="o">.</span><span class="n">quant_max</span><span class="p">,</span> <span class="n">tensor</span><span class="o">.</span><span class="n">zero_point_domain</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tensor</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="n">tensor</span><span class="o">.</span><span class="n">stride</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">return_and_correct_aliasing</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">new</span><span class="p">)</span>

<span class="nd">@implements</span><span class="p">(</span><span class="n">aten</span><span class="o">.</span><span class="n">slice</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">step</span> <span class="o">=</span> <span class="n">fill_defaults</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="k">assert</span> <span class="n">step</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="k">assert</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Only dim==0 or 1 are supported, got: </span><span class="si">{</span><span class="n">dim</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">if</span> <span class="n">end</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">dim</span><span class="p">]:</span>
        <span class="n">end</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">shape</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span>
    <span class="n">block_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">block_size</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Slice only works for 2d block_size right now, got: </span><span class="si">{</span><span class="n">block_size</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="c1"># with slice, some shape dimension might be smaller than block_size dimension, so</span>
    <span class="c1"># we need to make sure there is no overflow</span>
    <span class="n">block_size</span> <span class="o">=</span> <span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">block_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">min</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">block_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">new</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">(</span><span class="n">aten</span><span class="o">.</span><span class="n">slice</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tensor_impl</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">step</span><span class="p">),</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">quant_min</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">quant_max</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero_point_domain</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">return_and_correct_aliasing</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">new</span><span class="p">)</span>

<span class="c1"># this is needed for DTensor.from_local() and for flattening tensor</span>
<span class="nd">@implements</span><span class="p">(</span><span class="n">aten</span><span class="o">.</span><span class="n">view</span><span class="o">.</span><span class="n">default</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">shape</span> <span class="o">=</span> <span class="n">args</span>

    <span class="k">if</span> <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tensor_impl</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">quant_min</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">quant_max</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero_point_domain</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">())</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">block_size</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span>
        <span class="n">block_size</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">block_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tensor_impl</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">numel</span><span class="p">(),),</span> <span class="bp">self</span><span class="o">.</span><span class="n">quant_min</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">quant_max</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero_point_domain</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">())</span>

    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> only supports .view() with same shape or shape=[-1]&quot;</span><span class="p">)</span>


<span class="n">to_affine_quantized_intx</span> <span class="o">=</span> <span class="n">AffineQuantizedTensor</span><span class="o">.</span><span class="n">from_hp_to_intx</span>
<span class="n">to_affine_quantized_intx_static</span> <span class="o">=</span> <span class="n">AffineQuantizedTensor</span><span class="o">.</span><span class="n">from_hp_to_intx_static</span>
<span class="n">to_affine_quantized_floatx</span> <span class="o">=</span> <span class="n">AffineQuantizedTensor</span><span class="o">.</span><span class="n">from_hp_to_floatx</span>
<span class="n">to_affine_quantized_floatx_static</span> <span class="o">=</span> <span class="n">AffineQuantizedTensor</span><span class="o">.</span><span class="n">from_hp_to_floatx_static</span>
<span class="c1"># experimental will be merged in to floatx</span>
<span class="n">to_affine_quantized_fpx</span> <span class="o">=</span> <span class="n">AffineQuantizedTensor</span><span class="o">.</span><span class="n">from_hp_to_fpx</span>

<span class="k">if</span> <span class="n">TORCH_VERSION_AT_LEAST_2_5</span><span class="p">:</span>
    <span class="c1"># Allow a model with AffineQuantizedTensor weights to be loaded with `weights_only=True`</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">serialization</span><span class="o">.</span><span class="n">add_safe_globals</span><span class="p">([</span><span class="n">AffineQuantizedTensor</span><span class="p">])</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2024-present, torchao Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
         <script src="../../../_static/jquery.js"></script>
         <script src="../../../_static/underscore.js"></script>
         <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../_static/doctools.js"></script>
         <script src="../../../_static/clipboard.min.js"></script>
         <script src="../../../_static/copybutton.js"></script>
         <script src="../../../_static/design-tabs.js"></script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
<!-- Disabling "auto-collapsing" of sections on the left side bar. Replace script with commented out sections to reenable. -->
<!--  -->
<script script type="text/javascript">
    var collapsedSections = []
</script>

 
<script type="text/javascript">
  $(document).ready(function () {
    // Patch the "GitHub" link at the top of the page
    // to point to the torchao repo.
    var overwrite = function (_) {
      if ($(this).length > 0) {
        $(this)[0].href = "https://github.com/pytorch-labs/ao"
      }
    }
    // PC
    $(".main-menu a:contains('GitHub')").each(overwrite);
    // Mobile
    e$(".mobile-menu a:contains('Github')").each(overwrite);
  });
</script>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>Learn</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">Get Started</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">Tutorials</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>Ecosystem</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">Tools</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">Community</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">Forums</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">Developer Resources</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">Contributor Awards - 2023</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">About PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Docs</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch Domains</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>Blog & News</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch Blog</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">Community Blog</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">Videos</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>About</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">Governing Board</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>